{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "gE2aZnLqxvSZ",
    "outputId": "72034d93-d131-4682-aafa-0a25f522cfd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2oWWC1H0LMK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/Saki_2019/data/flair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1340
    },
    "colab_type": "code",
    "id": "eVCjf7mO0UNx",
    "outputId": "5aeff22d-fbd3-4d47-be62-c3e750387d1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/3a/2e777f65a71c1eaa259df44c44e39d7071ba8c7780a1564316a38bf86449/flair-0.4.2-py3-none-any.whl (136kB)\n",
      "\r",
      "\u001b[K     |██▍                             | 10kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 20kB 4.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 30kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 40kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 51kB 4.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 61kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 71kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 81kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 92kB 7.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 102kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 112kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 122kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 133kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 143kB 6.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
      "Collecting mpld3==0.3 (from flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 40.4MB/s \n",
      "\u001b[?25hCollecting regex (from flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 40.8MB/s \n",
      "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.1 (from flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 35.7MB/s \n",
      "\u001b[?25hCollecting segtok>=1.5.7 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.1.0)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n",
      "Collecting sqlitedict>=1.6.0 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.3)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Collecting deprecated>=1.2.4 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/7a/003fa432f1e45625626549726c2fbb7a29baa764e9d1fdb2323a5d779f8a/Deprecated-1.2.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
      "Collecting bpemb>=0.2.9 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (41.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.9.167)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.16.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.8.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.1)\n",
      "Collecting sentencepiece (from bpemb>=0.2.9->flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 41.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2.8)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (1.12.167)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.9.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.0)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.13.2)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.14)\n",
      "Building wheels for collected packages: mpld3, regex, segtok, sqlitedict\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
      "Successfully built mpld3 regex segtok sqlitedict\n",
      "Installing collected packages: mpld3, regex, pytorch-pretrained-bert, segtok, sqlitedict, deprecated, sentencepiece, bpemb, flair\n",
      "Successfully installed bpemb-0.3.0 deprecated-1.2.5 flair-0.4.2 mpld3-0.3 pytorch-pretrained-bert-0.6.2 regex-2019.6.8 segtok-1.5.7 sentencepiece-0.1.82 sqlitedict-1.6.0\n"
     ]
    }
   ],
   "source": [
    "# download flair library #\n",
    "! pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "KTBAKPlNzlb7",
    "outputId": "f7aef3c0-7dcd-47b5-fc7a-79ead9749153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 13:47:51,286 Reading data from /content/gdrive/My Drive/Saki_2019/data/flair\n",
      "2019-06-18 13:47:51,287 Train: /content/gdrive/My Drive/Saki_2019/data/flair/train_res_bilou.txt\n",
      "2019-06-18 13:47:51,292 Dev: None\n",
      "2019-06-18 13:47:51,293 Test: /content/gdrive/My Drive/Saki_2019/data/flair/test_res_bilou.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:312: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, column_format\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:318: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, column_format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 315 train + 35 dev + 87 test sentences\n",
      "[b'<unk>', b'O', b'O\"', b'Companies', b'-\"', b'Degree\"', b'Skills\"', b'<START>', b'<STOP>']\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from flair.datasets import Corpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "## make sure this describes your file structure\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# folder where training and test data are\n",
    "data_folder = '/content/gdrive/My Drive/Saki_2019/data/flair'\n",
    "\n",
    "# 1.0 is full data, try a much smaller number like 0.1 to test run the code\n",
    "downsample = 1 \n",
    "\n",
    "## your train file name\n",
    "train_file = 'train_res_bilou.txt'\n",
    "\n",
    "## your test file name\n",
    "test_file = 'test_res_bilou.txt'\n",
    "# 1. get the corpus\n",
    "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, train_file=train_file, test_file=test_file, dev_file=None).downsample(downsample)\n",
    "print(corpus)\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
    "print(tag_dictionary.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xBgwsFQR2HTC",
    "outputId": "b07db1f0-6193-4899-e0ac-ced20b181260"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# 4. initialize embeddings. Experiment with different embedding types to see what gets the best results\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings,FlairEmbeddings\n",
    "\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    WordEmbeddings('glove'),\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings (needs a LONG time to train :-)\n",
    "    #FlairEmbeddings('news-forward'),\n",
    "    #FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type='ner',\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 13355
    },
    "colab_type": "code",
    "id": "AmQOUUy52TQ6",
    "outputId": "0b259f8b-fced-4f4e-a2d2-660be5ced20c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 13:51:15,715 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:51:15,722 Evaluation method: MICRO_F1_SCORE\n",
      "2019-06-18 13:51:16,042 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:51:21,393 epoch 1 - iter 0/10 - loss 1982.46289062\n",
      "2019-06-18 13:51:29,960 epoch 1 - iter 1/10 - loss 2067.35180664\n",
      "2019-06-18 13:51:36,583 epoch 1 - iter 2/10 - loss 1802.03527832\n",
      "2019-06-18 13:51:40,370 epoch 1 - iter 3/10 - loss 1529.00665283\n",
      "2019-06-18 13:51:46,864 epoch 1 - iter 4/10 - loss 1346.66887207\n",
      "2019-06-18 13:51:52,464 epoch 1 - iter 5/10 - loss 1168.29695638\n",
      "2019-06-18 13:51:56,175 epoch 1 - iter 6/10 - loss 1028.02293614\n",
      "2019-06-18 13:52:00,096 epoch 1 - iter 7/10 - loss 929.84697723\n",
      "2019-06-18 13:52:04,829 epoch 1 - iter 8/10 - loss 851.04266018\n",
      "2019-06-18 13:52:08,346 epoch 1 - iter 9/10 - loss 782.64784088\n",
      "2019-06-18 13:52:08,854 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:52:08,855 EPOCH 1 done: loss 782.6478 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 13:52:21,923 DEV : loss 219.53799438476562 - score 0.9226\n",
      "2019-06-18 13:52:47,669 TEST : loss 208.9010009765625 - score 0.911\n",
      "2019-06-18 13:52:52,165 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:53:01,219 epoch 2 - iter 0/10 - loss 195.50628662\n",
      "2019-06-18 13:53:06,661 epoch 2 - iter 1/10 - loss 195.10069275\n",
      "2019-06-18 13:53:11,344 epoch 2 - iter 2/10 - loss 201.47170003\n",
      "2019-06-18 13:53:15,166 epoch 2 - iter 3/10 - loss 194.23944855\n",
      "2019-06-18 13:53:23,702 epoch 2 - iter 4/10 - loss 192.84328003\n",
      "2019-06-18 13:53:26,795 epoch 2 - iter 5/10 - loss 184.13968404\n",
      "2019-06-18 13:53:31,230 epoch 2 - iter 6/10 - loss 178.34671021\n",
      "2019-06-18 13:53:35,965 epoch 2 - iter 7/10 - loss 177.01230621\n",
      "2019-06-18 13:53:42,529 epoch 2 - iter 8/10 - loss 173.05206468\n",
      "2019-06-18 13:53:47,251 epoch 2 - iter 9/10 - loss 172.02225189\n",
      "2019-06-18 13:53:47,854 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:53:47,857 EPOCH 2 done: loss 172.0223 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 13:54:00,737 DEV : loss 174.85389709472656 - score 0.9226\n",
      "2019-06-18 13:54:26,458 TEST : loss 161.04164123535156 - score 0.911\n",
      "2019-06-18 13:54:30,859 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:54:40,252 epoch 3 - iter 0/10 - loss 155.37747192\n",
      "2019-06-18 13:54:45,655 epoch 3 - iter 1/10 - loss 141.78486633\n",
      "2019-06-18 13:54:49,949 epoch 3 - iter 2/10 - loss 143.57273356\n",
      "2019-06-18 13:54:53,570 epoch 3 - iter 3/10 - loss 142.34258270\n",
      "2019-06-18 13:54:58,313 epoch 3 - iter 4/10 - loss 142.83208618\n",
      "2019-06-18 13:55:02,623 epoch 3 - iter 5/10 - loss 139.26194509\n",
      "2019-06-18 13:55:11,613 epoch 3 - iter 6/10 - loss 140.83481707\n",
      "2019-06-18 13:55:16,168 epoch 3 - iter 7/10 - loss 139.75985718\n",
      "2019-06-18 13:55:20,951 epoch 3 - iter 8/10 - loss 141.35359870\n",
      "2019-06-18 13:55:25,435 epoch 3 - iter 9/10 - loss 139.56966934\n",
      "2019-06-18 13:55:26,013 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:55:26,018 EPOCH 3 done: loss 139.5697 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 13:55:38,809 DEV : loss 154.14816284179688 - score 0.9226\n",
      "2019-06-18 13:56:04,531 TEST : loss 138.65892028808594 - score 0.911\n",
      "2019-06-18 13:56:08,898 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:56:19,248 epoch 4 - iter 0/10 - loss 146.25051880\n",
      "2019-06-18 13:56:22,679 epoch 4 - iter 1/10 - loss 132.72871399\n",
      "2019-06-18 13:56:28,144 epoch 4 - iter 2/10 - loss 135.64006551\n",
      "2019-06-18 13:56:32,683 epoch 4 - iter 3/10 - loss 128.98313141\n",
      "2019-06-18 13:56:38,235 epoch 4 - iter 4/10 - loss 130.68686523\n",
      "2019-06-18 13:56:42,809 epoch 4 - iter 5/10 - loss 125.43727366\n",
      "2019-06-18 13:56:46,608 epoch 4 - iter 6/10 - loss 124.07646070\n",
      "2019-06-18 13:56:53,638 epoch 4 - iter 7/10 - loss 126.02252102\n",
      "2019-06-18 13:56:58,283 epoch 4 - iter 8/10 - loss 125.89522892\n",
      "2019-06-18 13:57:04,724 epoch 4 - iter 9/10 - loss 126.18888779\n",
      "2019-06-18 13:57:05,331 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:57:05,333 EPOCH 4 done: loss 126.1889 - lr 0.1000 - bad epochs 2\n",
      "2019-06-18 13:57:17,700 DEV : loss 142.69561767578125 - score 0.9226\n",
      "2019-06-18 13:57:43,562 TEST : loss 126.6151123046875 - score 0.911\n",
      "2019-06-18 13:57:47,891 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:57:53,726 epoch 5 - iter 0/10 - loss 121.78328705\n",
      "2019-06-18 13:58:00,458 epoch 5 - iter 1/10 - loss 117.53359222\n",
      "2019-06-18 13:58:05,092 epoch 5 - iter 2/10 - loss 121.65154012\n",
      "2019-06-18 13:58:10,541 epoch 5 - iter 3/10 - loss 120.65985870\n",
      "2019-06-18 13:58:16,829 epoch 5 - iter 4/10 - loss 118.90230103\n",
      "2019-06-18 13:58:21,537 epoch 5 - iter 5/10 - loss 118.83184052\n",
      "2019-06-18 13:58:28,818 epoch 5 - iter 6/10 - loss 118.68977574\n",
      "2019-06-18 13:58:32,598 epoch 5 - iter 7/10 - loss 117.41732025\n",
      "2019-06-18 13:58:39,263 epoch 5 - iter 8/10 - loss 115.36971961\n",
      "2019-06-18 13:58:44,101 epoch 5 - iter 9/10 - loss 116.74652634\n",
      "2019-06-18 13:58:44,713 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:58:44,715 EPOCH 5 done: loss 116.7465 - lr 0.1000 - bad epochs 3\n",
      "2019-06-18 13:58:57,378 DEV : loss 135.37379455566406 - score 0.9226\n",
      "2019-06-18 13:59:23,061 TEST : loss 119.60678100585938 - score 0.911\n",
      "Epoch     4: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-06-18 13:59:27,414 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 13:59:33,526 epoch 6 - iter 0/10 - loss 107.69199371\n",
      "2019-06-18 13:59:42,390 epoch 6 - iter 1/10 - loss 107.98939133\n",
      "2019-06-18 13:59:48,730 epoch 6 - iter 2/10 - loss 109.00158183\n",
      "2019-06-18 13:59:53,579 epoch 6 - iter 3/10 - loss 111.68751526\n",
      "2019-06-18 13:59:57,417 epoch 6 - iter 4/10 - loss 109.44071045\n",
      "2019-06-18 14:00:02,167 epoch 6 - iter 5/10 - loss 109.82813390\n",
      "2019-06-18 14:00:06,607 epoch 6 - iter 6/10 - loss 108.21665192\n",
      "2019-06-18 14:00:12,143 epoch 6 - iter 7/10 - loss 107.77721786\n",
      "2019-06-18 14:00:16,815 epoch 6 - iter 8/10 - loss 106.41843160\n",
      "2019-06-18 14:00:22,960 epoch 6 - iter 9/10 - loss 104.90873566\n",
      "2019-06-18 14:00:23,578 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:00:23,580 EPOCH 6 done: loss 104.9087 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 14:00:36,922 DEV : loss 119.52230834960938 - score 0.9226\n",
      "2019-06-18 14:01:02,664 TEST : loss 99.15321350097656 - score 0.911\n",
      "2019-06-18 14:01:06,998 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:01:15,008 epoch 7 - iter 0/10 - loss 102.78921509\n",
      "2019-06-18 14:01:20,850 epoch 7 - iter 1/10 - loss 104.02408600\n",
      "2019-06-18 14:01:26,949 epoch 7 - iter 2/10 - loss 100.61752319\n",
      "2019-06-18 14:01:33,330 epoch 7 - iter 3/10 - loss 96.33544350\n",
      "2019-06-18 14:01:38,114 epoch 7 - iter 4/10 - loss 96.96230621\n",
      "2019-06-18 14:01:42,908 epoch 7 - iter 5/10 - loss 100.76337306\n",
      "2019-06-18 14:01:47,709 epoch 7 - iter 6/10 - loss 100.97068678\n",
      "2019-06-18 14:01:53,192 epoch 7 - iter 7/10 - loss 102.55469036\n",
      "2019-06-18 14:01:59,771 epoch 7 - iter 8/10 - loss 102.21166568\n",
      "2019-06-18 14:02:04,075 epoch 7 - iter 9/10 - loss 101.50460129\n",
      "2019-06-18 14:02:04,694 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:02:04,696 EPOCH 7 done: loss 101.5046 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 14:02:18,219 DEV : loss 115.74935150146484 - score 0.9226\n",
      "2019-06-18 14:02:43,981 TEST : loss 96.12956237792969 - score 0.911\n",
      "2019-06-18 14:02:48,343 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:02:56,390 epoch 8 - iter 0/10 - loss 93.59558105\n",
      "2019-06-18 14:03:03,255 epoch 8 - iter 1/10 - loss 97.68957520\n",
      "2019-06-18 14:03:08,978 epoch 8 - iter 2/10 - loss 99.83049774\n",
      "2019-06-18 14:03:12,850 epoch 8 - iter 3/10 - loss 100.74329567\n",
      "2019-06-18 14:03:16,153 epoch 8 - iter 4/10 - loss 98.80069122\n",
      "2019-06-18 14:03:23,227 epoch 8 - iter 5/10 - loss 100.92941411\n",
      "2019-06-18 14:03:27,958 epoch 8 - iter 6/10 - loss 101.13078962\n",
      "2019-06-18 14:03:32,431 epoch 8 - iter 7/10 - loss 99.36040115\n",
      "2019-06-18 14:03:36,308 epoch 8 - iter 8/10 - loss 98.18770176\n",
      "2019-06-18 14:03:43,907 epoch 8 - iter 9/10 - loss 99.19527893\n",
      "2019-06-18 14:03:44,531 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:03:44,532 EPOCH 8 done: loss 99.1953 - lr 0.0500 - bad epochs 2\n",
      "2019-06-18 14:03:55,566 DEV : loss 112.41476440429688 - score 0.9226\n",
      "2019-06-18 14:04:21,113 TEST : loss 93.36015319824219 - score 0.911\n",
      "2019-06-18 14:04:25,440 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:04:33,317 epoch 9 - iter 0/10 - loss 97.26728058\n",
      "2019-06-18 14:04:40,701 epoch 9 - iter 1/10 - loss 104.96017456\n",
      "2019-06-18 14:04:44,556 epoch 9 - iter 2/10 - loss 99.00406901\n",
      "2019-06-18 14:04:47,661 epoch 9 - iter 3/10 - loss 99.48121071\n",
      "2019-06-18 14:04:51,428 epoch 9 - iter 4/10 - loss 96.11649933\n",
      "2019-06-18 14:04:56,089 epoch 9 - iter 5/10 - loss 95.69827652\n",
      "2019-06-18 14:05:01,054 epoch 9 - iter 6/10 - loss 95.98739406\n",
      "2019-06-18 14:05:07,659 epoch 9 - iter 7/10 - loss 96.67055416\n",
      "2019-06-18 14:05:14,516 epoch 9 - iter 8/10 - loss 97.93138207\n",
      "2019-06-18 14:05:20,854 epoch 9 - iter 9/10 - loss 97.04779434\n",
      "2019-06-18 14:05:21,483 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:05:21,486 EPOCH 9 done: loss 97.0478 - lr 0.0500 - bad epochs 3\n",
      "2019-06-18 14:05:32,518 DEV : loss 111.38017272949219 - score 0.9226\n",
      "2019-06-18 14:05:58,152 TEST : loss 92.64102172851562 - score 0.911\n",
      "Epoch     8: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2019-06-18 14:06:02,551 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:06:08,724 epoch 10 - iter 0/10 - loss 90.51614380\n",
      "2019-06-18 14:06:14,426 epoch 10 - iter 1/10 - loss 93.38757324\n",
      "2019-06-18 14:06:17,860 epoch 10 - iter 2/10 - loss 94.69404602\n",
      "2019-06-18 14:06:24,224 epoch 10 - iter 3/10 - loss 95.07195282\n",
      "2019-06-18 14:06:30,594 epoch 10 - iter 4/10 - loss 94.08415680\n",
      "2019-06-18 14:06:35,225 epoch 10 - iter 5/10 - loss 92.11005656\n",
      "2019-06-18 14:06:41,830 epoch 10 - iter 6/10 - loss 91.58972713\n",
      "2019-06-18 14:06:47,457 epoch 10 - iter 7/10 - loss 93.44556046\n",
      "2019-06-18 14:06:53,267 epoch 10 - iter 8/10 - loss 94.23236847\n",
      "2019-06-18 14:06:57,604 epoch 10 - iter 9/10 - loss 93.21143188\n",
      "2019-06-18 14:06:58,225 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:06:58,226 EPOCH 10 done: loss 93.2114 - lr 0.0250 - bad epochs 0\n",
      "2019-06-18 14:07:09,351 DEV : loss 108.54171752929688 - score 0.9226\n",
      "2019-06-18 14:07:34,850 TEST : loss 90.37846374511719 - score 0.911\n",
      "2019-06-18 14:07:39,207 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:07:46,673 epoch 11 - iter 0/10 - loss 86.37584686\n",
      "2019-06-18 14:07:50,039 epoch 11 - iter 1/10 - loss 86.72422028\n",
      "2019-06-18 14:07:55,530 epoch 11 - iter 2/10 - loss 86.50576782\n",
      "2019-06-18 14:08:01,531 epoch 11 - iter 3/10 - loss 88.89252472\n",
      "2019-06-18 14:08:06,323 epoch 11 - iter 4/10 - loss 89.64267426\n",
      "2019-06-18 14:08:10,119 epoch 11 - iter 5/10 - loss 91.63400777\n",
      "2019-06-18 14:08:18,526 epoch 11 - iter 6/10 - loss 93.60730743\n",
      "2019-06-18 14:08:23,958 epoch 11 - iter 7/10 - loss 93.28134632\n",
      "2019-06-18 14:08:28,805 epoch 11 - iter 8/10 - loss 92.35028500\n",
      "2019-06-18 14:08:32,469 epoch 11 - iter 9/10 - loss 92.36696701\n",
      "2019-06-18 14:08:33,075 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:08:33,080 EPOCH 11 done: loss 92.3670 - lr 0.0250 - bad epochs 1\n",
      "2019-06-18 14:08:45,713 DEV : loss 107.638427734375 - score 0.9226\n",
      "2019-06-18 14:09:11,358 TEST : loss 89.60929870605469 - score 0.911\n",
      "2019-06-18 14:09:15,712 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:09:20,933 epoch 12 - iter 0/10 - loss 76.67318726\n",
      "2019-06-18 14:09:29,009 epoch 12 - iter 1/10 - loss 84.36838913\n",
      "2019-06-18 14:09:34,113 epoch 12 - iter 2/10 - loss 92.58212789\n",
      "2019-06-18 14:09:40,693 epoch 12 - iter 3/10 - loss 90.62533760\n",
      "2019-06-18 14:09:48,757 epoch 12 - iter 4/10 - loss 91.29902802\n",
      "2019-06-18 14:09:52,079 epoch 12 - iter 5/10 - loss 92.78521601\n",
      "2019-06-18 14:09:56,747 epoch 12 - iter 6/10 - loss 94.94142696\n",
      "2019-06-18 14:10:00,402 epoch 12 - iter 7/10 - loss 92.85523510\n",
      "2019-06-18 14:10:04,389 epoch 12 - iter 8/10 - loss 92.38925425\n",
      "2019-06-18 14:10:08,690 epoch 12 - iter 9/10 - loss 91.59069366\n",
      "2019-06-18 14:10:09,315 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:10:09,317 EPOCH 12 done: loss 91.5907 - lr 0.0250 - bad epochs 2\n",
      "2019-06-18 14:10:22,273 DEV : loss 106.34568786621094 - score 0.9226\n",
      "2019-06-18 14:10:48,160 TEST : loss 88.3946762084961 - score 0.911\n",
      "2019-06-18 14:10:52,465 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:11:00,738 epoch 13 - iter 0/10 - loss 106.80768585\n",
      "2019-06-18 14:11:07,313 epoch 13 - iter 1/10 - loss 100.21231842\n",
      "2019-06-18 14:11:12,095 epoch 13 - iter 2/10 - loss 98.64331309\n",
      "2019-06-18 14:11:18,731 epoch 13 - iter 3/10 - loss 96.75728798\n",
      "2019-06-18 14:11:24,297 epoch 13 - iter 4/10 - loss 94.25644073\n",
      "2019-06-18 14:11:29,528 epoch 13 - iter 5/10 - loss 91.98283641\n",
      "2019-06-18 14:11:32,967 epoch 13 - iter 6/10 - loss 90.19653211\n",
      "2019-06-18 14:11:39,547 epoch 13 - iter 7/10 - loss 90.88803387\n",
      "2019-06-18 14:11:44,296 epoch 13 - iter 8/10 - loss 90.98345015\n",
      "2019-06-18 14:11:48,002 epoch 13 - iter 9/10 - loss 90.97601242\n",
      "2019-06-18 14:11:48,607 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:11:48,609 EPOCH 13 done: loss 90.9760 - lr 0.0250 - bad epochs 3\n",
      "2019-06-18 14:12:00,889 DEV : loss 106.237060546875 - score 0.9226\n",
      "2019-06-18 14:12:26,583 TEST : loss 88.12874603271484 - score 0.911\n",
      "Epoch    12: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2019-06-18 14:12:30,925 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:12:37,223 epoch 14 - iter 0/10 - loss 89.78958130\n",
      "2019-06-18 14:12:42,050 epoch 14 - iter 1/10 - loss 86.43316269\n",
      "2019-06-18 14:12:50,073 epoch 14 - iter 2/10 - loss 86.13562266\n",
      "2019-06-18 14:12:57,011 epoch 14 - iter 3/10 - loss 89.38119698\n",
      "2019-06-18 14:13:01,733 epoch 14 - iter 4/10 - loss 93.77351227\n",
      "2019-06-18 14:13:06,239 epoch 14 - iter 5/10 - loss 91.71481069\n",
      "2019-06-18 14:13:11,697 epoch 14 - iter 6/10 - loss 90.39558193\n",
      "2019-06-18 14:13:18,457 epoch 14 - iter 7/10 - loss 89.56348610\n",
      "2019-06-18 14:13:22,296 epoch 14 - iter 8/10 - loss 88.85804409\n",
      "2019-06-18 14:13:27,014 epoch 14 - iter 9/10 - loss 89.41272202\n",
      "2019-06-18 14:13:27,647 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:13:27,651 EPOCH 14 done: loss 89.4127 - lr 0.0125 - bad epochs 0\n",
      "2019-06-18 14:13:40,582 DEV : loss 104.33500671386719 - score 0.9226\n",
      "2019-06-18 14:14:06,013 TEST : loss 87.20220947265625 - score 0.911\n",
      "2019-06-18 14:14:10,368 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:14:16,381 epoch 15 - iter 0/10 - loss 87.04141235\n",
      "2019-06-18 14:14:23,297 epoch 15 - iter 1/10 - loss 90.16297150\n",
      "2019-06-18 14:14:28,608 epoch 15 - iter 2/10 - loss 87.50106812\n",
      "2019-06-18 14:14:33,583 epoch 15 - iter 3/10 - loss 89.11520576\n",
      "2019-06-18 14:14:38,374 epoch 15 - iter 4/10 - loss 89.83728485\n",
      "2019-06-18 14:14:46,492 epoch 15 - iter 5/10 - loss 88.26718521\n",
      "2019-06-18 14:14:52,915 epoch 15 - iter 6/10 - loss 88.91567230\n",
      "2019-06-18 14:14:58,348 epoch 15 - iter 7/10 - loss 89.02993202\n",
      "2019-06-18 14:15:02,340 epoch 15 - iter 8/10 - loss 88.81578742\n",
      "2019-06-18 14:15:06,873 epoch 15 - iter 9/10 - loss 89.19360580\n",
      "2019-06-18 14:15:07,479 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:15:07,485 EPOCH 15 done: loss 89.1936 - lr 0.0125 - bad epochs 1\n",
      "2019-06-18 14:15:18,501 DEV : loss 103.48675537109375 - score 0.9226\n",
      "2019-06-18 14:15:44,080 TEST : loss 86.16900634765625 - score 0.911\n",
      "2019-06-18 14:15:48,422 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:15:56,403 epoch 16 - iter 0/10 - loss 89.03157043\n",
      "2019-06-18 14:16:01,298 epoch 16 - iter 1/10 - loss 83.84853363\n",
      "2019-06-18 14:16:05,664 epoch 16 - iter 2/10 - loss 82.08676147\n",
      "2019-06-18 14:16:11,335 epoch 16 - iter 3/10 - loss 86.08451462\n",
      "2019-06-18 14:16:16,111 epoch 16 - iter 4/10 - loss 86.97557983\n",
      "2019-06-18 14:16:21,713 epoch 16 - iter 5/10 - loss 89.14937337\n",
      "2019-06-18 14:16:27,167 epoch 16 - iter 6/10 - loss 89.60623278\n",
      "2019-06-18 14:16:30,959 epoch 16 - iter 7/10 - loss 88.59067726\n",
      "2019-06-18 14:16:39,343 epoch 16 - iter 8/10 - loss 88.69017877\n",
      "2019-06-18 14:16:43,638 epoch 16 - iter 9/10 - loss 88.61053467\n",
      "2019-06-18 14:16:44,245 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:16:44,251 EPOCH 16 done: loss 88.6105 - lr 0.0125 - bad epochs 2\n",
      "2019-06-18 14:16:55,171 DEV : loss 103.76133728027344 - score 0.9226\n",
      "2019-06-18 14:17:21,832 TEST : loss 86.15413665771484 - score 0.911\n",
      "2019-06-18 14:17:26,149 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:17:35,237 epoch 17 - iter 0/10 - loss 98.50737000\n",
      "2019-06-18 14:17:39,716 epoch 17 - iter 1/10 - loss 91.57899857\n",
      "2019-06-18 14:17:44,275 epoch 17 - iter 2/10 - loss 89.94486237\n",
      "2019-06-18 14:17:49,538 epoch 17 - iter 3/10 - loss 88.32052422\n",
      "2019-06-18 14:17:53,440 epoch 17 - iter 4/10 - loss 87.60526276\n",
      "2019-06-18 14:18:00,123 epoch 17 - iter 5/10 - loss 90.31831233\n",
      "2019-06-18 14:18:06,138 epoch 17 - iter 6/10 - loss 90.33710044\n",
      "2019-06-18 14:18:12,767 epoch 17 - iter 7/10 - loss 91.77148342\n",
      "2019-06-18 14:18:16,568 epoch 17 - iter 8/10 - loss 89.79886373\n",
      "2019-06-18 14:18:19,988 epoch 17 - iter 9/10 - loss 87.86444016\n",
      "2019-06-18 14:18:20,608 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:18:20,611 EPOCH 17 done: loss 87.8644 - lr 0.0125 - bad epochs 3\n",
      "2019-06-18 14:18:32,982 DEV : loss 103.6114273071289 - score 0.9226\n",
      "2019-06-18 14:18:58,739 TEST : loss 87.30516815185547 - score 0.911\n",
      "Epoch    16: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2019-06-18 14:19:03,065 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:19:09,141 epoch 18 - iter 0/10 - loss 95.24903107\n",
      "2019-06-18 14:19:15,559 epoch 18 - iter 1/10 - loss 90.95101166\n",
      "2019-06-18 14:19:21,038 epoch 18 - iter 2/10 - loss 91.36915080\n",
      "2019-06-18 14:19:26,794 epoch 18 - iter 3/10 - loss 87.33699608\n",
      "2019-06-18 14:19:33,289 epoch 18 - iter 4/10 - loss 87.95042877\n",
      "2019-06-18 14:19:39,417 epoch 18 - iter 5/10 - loss 88.75537999\n",
      "2019-06-18 14:19:44,133 epoch 18 - iter 6/10 - loss 89.15636226\n",
      "2019-06-18 14:19:48,510 epoch 18 - iter 7/10 - loss 88.57576752\n",
      "2019-06-18 14:19:54,988 epoch 18 - iter 8/10 - loss 87.95286221\n",
      "2019-06-18 14:19:58,814 epoch 18 - iter 9/10 - loss 87.73026276\n",
      "2019-06-18 14:19:59,444 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:19:59,446 EPOCH 18 done: loss 87.7303 - lr 0.0063 - bad epochs 0\n",
      "2019-06-18 14:20:12,284 DEV : loss 102.40513610839844 - score 0.9226\n",
      "2019-06-18 14:20:38,077 TEST : loss 85.19758605957031 - score 0.911\n",
      "2019-06-18 14:20:42,374 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:20:50,898 epoch 19 - iter 0/10 - loss 86.29642487\n",
      "2019-06-18 14:20:56,529 epoch 19 - iter 1/10 - loss 94.58880615\n",
      "2019-06-18 14:21:02,100 epoch 19 - iter 2/10 - loss 93.94969686\n",
      "2019-06-18 14:21:08,455 epoch 19 - iter 3/10 - loss 90.03922272\n",
      "2019-06-18 14:21:14,652 epoch 19 - iter 4/10 - loss 87.57580872\n",
      "2019-06-18 14:21:19,377 epoch 19 - iter 5/10 - loss 88.29165649\n",
      "2019-06-18 14:21:24,073 epoch 19 - iter 6/10 - loss 87.72408840\n",
      "2019-06-18 14:21:28,670 epoch 19 - iter 7/10 - loss 88.45820713\n",
      "2019-06-18 14:21:34,155 epoch 19 - iter 8/10 - loss 88.35138363\n",
      "2019-06-18 14:21:37,260 epoch 19 - iter 9/10 - loss 87.24821320\n",
      "2019-06-18 14:21:37,880 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:21:37,885 EPOCH 19 done: loss 87.2482 - lr 0.0063 - bad epochs 1\n",
      "2019-06-18 14:21:51,205 DEV : loss 102.45451354980469 - score 0.9226\n",
      "2019-06-18 14:22:17,118 TEST : loss 85.20169067382812 - score 0.911\n",
      "2019-06-18 14:22:21,506 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:22:29,952 epoch 20 - iter 0/10 - loss 90.77645874\n",
      "2019-06-18 14:22:37,170 epoch 20 - iter 1/10 - loss 97.10779572\n",
      "2019-06-18 14:22:42,429 epoch 20 - iter 2/10 - loss 96.31743113\n",
      "2019-06-18 14:22:48,037 epoch 20 - iter 3/10 - loss 91.52788162\n",
      "2019-06-18 14:22:52,676 epoch 20 - iter 4/10 - loss 91.43834686\n",
      "2019-06-18 14:22:56,949 epoch 20 - iter 5/10 - loss 89.17537689\n",
      "2019-06-18 14:23:00,545 epoch 20 - iter 6/10 - loss 88.72717721\n",
      "2019-06-18 14:23:04,431 epoch 20 - iter 7/10 - loss 88.95960808\n",
      "2019-06-18 14:23:08,905 epoch 20 - iter 8/10 - loss 87.11289978\n",
      "2019-06-18 14:23:15,239 epoch 20 - iter 9/10 - loss 87.11863251\n",
      "2019-06-18 14:23:15,852 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:23:15,854 EPOCH 20 done: loss 87.1186 - lr 0.0063 - bad epochs 2\n",
      "2019-06-18 14:23:28,920 DEV : loss 101.9460678100586 - score 0.9226\n",
      "2019-06-18 14:23:54,622 TEST : loss 85.0108642578125 - score 0.911\n",
      "2019-06-18 14:23:58,932 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:24:07,410 epoch 21 - iter 0/10 - loss 87.03843689\n",
      "2019-06-18 14:24:13,724 epoch 21 - iter 1/10 - loss 88.38828278\n",
      "2019-06-18 14:24:18,222 epoch 21 - iter 2/10 - loss 85.68872325\n",
      "2019-06-18 14:24:25,095 epoch 21 - iter 3/10 - loss 87.27276421\n",
      "2019-06-18 14:24:29,615 epoch 21 - iter 4/10 - loss 87.92817230\n",
      "2019-06-18 14:24:36,005 epoch 21 - iter 5/10 - loss 86.93985113\n",
      "2019-06-18 14:24:38,948 epoch 21 - iter 6/10 - loss 85.67157200\n",
      "2019-06-18 14:24:43,643 epoch 21 - iter 7/10 - loss 86.12463570\n",
      "2019-06-18 14:24:48,258 epoch 21 - iter 8/10 - loss 85.88536072\n",
      "2019-06-18 14:24:53,610 epoch 21 - iter 9/10 - loss 87.32370911\n",
      "2019-06-18 14:24:54,231 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:24:54,233 EPOCH 21 done: loss 87.3237 - lr 0.0063 - bad epochs 3\n",
      "2019-06-18 14:25:07,312 DEV : loss 101.89410400390625 - score 0.9226\n",
      "2019-06-18 14:25:32,784 TEST : loss 85.12666320800781 - score 0.911\n",
      "Epoch    20: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2019-06-18 14:25:37,185 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:25:46,231 epoch 22 - iter 0/10 - loss 105.70808411\n",
      "2019-06-18 14:25:51,718 epoch 22 - iter 1/10 - loss 91.66640472\n",
      "2019-06-18 14:25:55,477 epoch 22 - iter 2/10 - loss 87.76155090\n",
      "2019-06-18 14:26:00,688 epoch 22 - iter 3/10 - loss 85.51810646\n",
      "2019-06-18 14:26:06,965 epoch 22 - iter 4/10 - loss 85.09432220\n",
      "2019-06-18 14:26:13,475 epoch 22 - iter 5/10 - loss 84.12282054\n",
      "2019-06-18 14:26:18,059 epoch 22 - iter 6/10 - loss 84.81708091\n",
      "2019-06-18 14:26:21,750 epoch 22 - iter 7/10 - loss 84.50814533\n",
      "2019-06-18 14:26:26,732 epoch 22 - iter 8/10 - loss 86.59457652\n",
      "2019-06-18 14:26:33,176 epoch 22 - iter 9/10 - loss 86.81343231\n",
      "2019-06-18 14:26:33,798 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:26:33,800 EPOCH 22 done: loss 86.8134 - lr 0.0031 - bad epochs 0\n",
      "2019-06-18 14:26:46,723 DEV : loss 101.57989501953125 - score 0.9226\n",
      "2019-06-18 14:27:12,393 TEST : loss 84.64962005615234 - score 0.911\n",
      "2019-06-18 14:27:16,710 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:27:22,778 epoch 23 - iter 0/10 - loss 77.97453308\n",
      "2019-06-18 14:27:30,405 epoch 23 - iter 1/10 - loss 85.52009201\n",
      "2019-06-18 14:27:35,018 epoch 23 - iter 2/10 - loss 86.37178294\n",
      "2019-06-18 14:27:38,784 epoch 23 - iter 3/10 - loss 85.91458130\n",
      "2019-06-18 14:27:42,834 epoch 23 - iter 4/10 - loss 85.24342957\n",
      "2019-06-18 14:27:47,430 epoch 23 - iter 5/10 - loss 84.01931763\n",
      "2019-06-18 14:27:55,560 epoch 23 - iter 6/10 - loss 86.95470755\n",
      "2019-06-18 14:28:02,017 epoch 23 - iter 7/10 - loss 86.03824234\n",
      "2019-06-18 14:28:06,318 epoch 23 - iter 8/10 - loss 85.71610345\n",
      "2019-06-18 14:28:11,087 epoch 23 - iter 9/10 - loss 86.95383987\n",
      "2019-06-18 14:28:11,686 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:28:11,688 EPOCH 23 done: loss 86.9538 - lr 0.0031 - bad epochs 1\n",
      "2019-06-18 14:28:24,287 DEV : loss 101.51654052734375 - score 0.9226\n",
      "2019-06-18 14:28:49,837 TEST : loss 84.57195281982422 - score 0.911\n",
      "2019-06-18 14:28:54,148 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:28:59,758 epoch 24 - iter 0/10 - loss 94.55215454\n",
      "2019-06-18 14:29:06,696 epoch 24 - iter 1/10 - loss 96.62006378\n",
      "2019-06-18 14:29:11,472 epoch 24 - iter 2/10 - loss 95.99368032\n",
      "2019-06-18 14:29:14,518 epoch 24 - iter 3/10 - loss 89.54132271\n",
      "2019-06-18 14:29:22,484 epoch 24 - iter 4/10 - loss 90.36419220\n",
      "2019-06-18 14:29:29,193 epoch 24 - iter 5/10 - loss 88.46710841\n",
      "2019-06-18 14:29:32,092 epoch 24 - iter 6/10 - loss 88.13866425\n",
      "2019-06-18 14:29:36,370 epoch 24 - iter 7/10 - loss 87.85382938\n",
      "2019-06-18 14:29:41,157 epoch 24 - iter 8/10 - loss 87.31975895\n",
      "2019-06-18 14:29:47,052 epoch 24 - iter 9/10 - loss 86.45633850\n",
      "2019-06-18 14:29:47,659 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:29:47,662 EPOCH 24 done: loss 86.4563 - lr 0.0031 - bad epochs 2\n",
      "2019-06-18 14:29:58,682 DEV : loss 101.40319061279297 - score 0.9226\n",
      "2019-06-18 14:30:24,345 TEST : loss 84.36480712890625 - score 0.911\n",
      "2019-06-18 14:30:28,742 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:30:34,758 epoch 25 - iter 0/10 - loss 82.23085022\n",
      "2019-06-18 14:30:44,903 epoch 25 - iter 1/10 - loss 92.31774902\n",
      "2019-06-18 14:30:49,648 epoch 25 - iter 2/10 - loss 90.28285726\n",
      "2019-06-18 14:30:53,496 epoch 25 - iter 3/10 - loss 86.51584625\n",
      "2019-06-18 14:30:56,651 epoch 25 - iter 4/10 - loss 86.23938599\n",
      "2019-06-18 14:31:02,016 epoch 25 - iter 5/10 - loss 84.46600342\n",
      "2019-06-18 14:31:08,617 epoch 25 - iter 6/10 - loss 84.26959229\n",
      "2019-06-18 14:31:13,238 epoch 25 - iter 7/10 - loss 86.13312149\n",
      "2019-06-18 14:31:18,161 epoch 25 - iter 8/10 - loss 87.76056332\n",
      "2019-06-18 14:31:22,891 epoch 25 - iter 9/10 - loss 86.34131088\n",
      "2019-06-18 14:31:23,537 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:31:23,542 EPOCH 25 done: loss 86.3413 - lr 0.0031 - bad epochs 3\n",
      "2019-06-18 14:31:34,574 DEV : loss 101.3834228515625 - score 0.9226\n",
      "2019-06-18 14:32:00,203 TEST : loss 84.346923828125 - score 0.911\n",
      "Epoch    24: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2019-06-18 14:32:04,494 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:32:14,762 epoch 26 - iter 0/10 - loss 77.32356262\n",
      "2019-06-18 14:32:19,539 epoch 26 - iter 1/10 - loss 80.90075684\n",
      "2019-06-18 14:32:24,094 epoch 26 - iter 2/10 - loss 85.78294881\n",
      "2019-06-18 14:32:29,886 epoch 26 - iter 3/10 - loss 83.72037315\n",
      "2019-06-18 14:32:35,631 epoch 26 - iter 4/10 - loss 87.76203308\n",
      "2019-06-18 14:32:40,144 epoch 26 - iter 5/10 - loss 88.53817495\n",
      "2019-06-18 14:32:46,766 epoch 26 - iter 6/10 - loss 87.75917707\n",
      "2019-06-18 14:32:52,164 epoch 26 - iter 7/10 - loss 87.40411568\n",
      "2019-06-18 14:32:58,294 epoch 26 - iter 8/10 - loss 87.84929064\n",
      "2019-06-18 14:33:01,843 epoch 26 - iter 9/10 - loss 86.15289154\n",
      "2019-06-18 14:33:02,485 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:33:02,487 EPOCH 26 done: loss 86.1529 - lr 0.0016 - bad epochs 0\n",
      "2019-06-18 14:33:13,509 DEV : loss 101.22000885009766 - score 0.9226\n",
      "2019-06-18 14:33:39,383 TEST : loss 84.31857299804688 - score 0.911\n",
      "2019-06-18 14:33:43,652 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:33:52,368 epoch 27 - iter 0/10 - loss 91.22357178\n",
      "2019-06-18 14:33:59,111 epoch 27 - iter 1/10 - loss 90.85153198\n",
      "2019-06-18 14:34:06,292 epoch 27 - iter 2/10 - loss 94.83213806\n",
      "2019-06-18 14:34:11,039 epoch 27 - iter 3/10 - loss 92.34077072\n",
      "2019-06-18 14:34:14,162 epoch 27 - iter 4/10 - loss 88.41804504\n",
      "2019-06-18 14:34:17,992 epoch 27 - iter 5/10 - loss 88.11283366\n",
      "2019-06-18 14:34:22,205 epoch 27 - iter 6/10 - loss 88.00514766\n",
      "2019-06-18 14:34:28,780 epoch 27 - iter 7/10 - loss 87.96462536\n",
      "2019-06-18 14:34:31,702 epoch 27 - iter 8/10 - loss 86.86089749\n",
      "2019-06-18 14:34:36,937 epoch 27 - iter 9/10 - loss 86.30439606\n",
      "2019-06-18 14:34:37,548 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:34:37,550 EPOCH 27 done: loss 86.3044 - lr 0.0016 - bad epochs 1\n",
      "2019-06-18 14:34:48,591 DEV : loss 101.1980209350586 - score 0.9226\n",
      "2019-06-18 14:35:15,208 TEST : loss 84.15704345703125 - score 0.911\n",
      "2019-06-18 14:35:19,557 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:35:25,293 epoch 28 - iter 0/10 - loss 95.12848663\n",
      "2019-06-18 14:35:32,371 epoch 28 - iter 1/10 - loss 93.71972275\n",
      "2019-06-18 14:35:35,990 epoch 28 - iter 2/10 - loss 91.46039073\n",
      "2019-06-18 14:35:40,471 epoch 28 - iter 3/10 - loss 87.86783791\n",
      "2019-06-18 14:35:45,964 epoch 28 - iter 4/10 - loss 87.94378204\n",
      "2019-06-18 14:35:51,909 epoch 28 - iter 5/10 - loss 87.24632390\n",
      "2019-06-18 14:35:54,966 epoch 28 - iter 6/10 - loss 87.62298584\n",
      "2019-06-18 14:36:01,456 epoch 28 - iter 7/10 - loss 87.35127640\n",
      "2019-06-18 14:36:05,862 epoch 28 - iter 8/10 - loss 87.14617666\n",
      "2019-06-18 14:36:12,290 epoch 28 - iter 9/10 - loss 86.23973160\n",
      "2019-06-18 14:36:12,900 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:36:12,902 EPOCH 28 done: loss 86.2397 - lr 0.0016 - bad epochs 2\n",
      "2019-06-18 14:36:25,638 DEV : loss 101.23973083496094 - score 0.9226\n",
      "2019-06-18 14:36:51,576 TEST : loss 84.17213439941406 - score 0.911\n",
      "2019-06-18 14:36:55,891 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:37:06,337 epoch 29 - iter 0/10 - loss 107.53936768\n",
      "2019-06-18 14:37:10,770 epoch 29 - iter 1/10 - loss 89.63375092\n",
      "2019-06-18 14:37:14,670 epoch 29 - iter 2/10 - loss 86.46063232\n",
      "2019-06-18 14:37:18,644 epoch 29 - iter 3/10 - loss 84.52993011\n",
      "2019-06-18 14:37:27,104 epoch 29 - iter 4/10 - loss 88.83890991\n",
      "2019-06-18 14:37:32,732 epoch 29 - iter 5/10 - loss 88.49137878\n",
      "2019-06-18 14:37:36,278 epoch 29 - iter 6/10 - loss 86.09771293\n",
      "2019-06-18 14:37:40,667 epoch 29 - iter 7/10 - loss 85.37659073\n",
      "2019-06-18 14:37:45,288 epoch 29 - iter 8/10 - loss 85.67811584\n",
      "2019-06-18 14:37:49,469 epoch 29 - iter 9/10 - loss 86.33536453\n",
      "2019-06-18 14:37:50,066 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:37:50,071 EPOCH 29 done: loss 86.3354 - lr 0.0016 - bad epochs 3\n",
      "2019-06-18 14:38:03,324 DEV : loss 101.247802734375 - score 0.9226\n",
      "2019-06-18 14:38:29,175 TEST : loss 84.06719970703125 - score 0.911\n",
      "Epoch    28: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2019-06-18 14:38:33,483 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:38:43,892 epoch 30 - iter 0/10 - loss 101.02226257\n",
      "2019-06-18 14:38:49,491 epoch 30 - iter 1/10 - loss 92.14241028\n",
      "2019-06-18 14:38:54,115 epoch 30 - iter 2/10 - loss 91.21103414\n",
      "2019-06-18 14:38:59,797 epoch 30 - iter 3/10 - loss 89.05147552\n",
      "2019-06-18 14:39:04,178 epoch 30 - iter 4/10 - loss 87.59983368\n",
      "2019-06-18 14:39:10,221 epoch 30 - iter 5/10 - loss 86.85332108\n",
      "2019-06-18 14:39:16,786 epoch 30 - iter 6/10 - loss 87.64341409\n",
      "2019-06-18 14:39:21,295 epoch 30 - iter 7/10 - loss 87.59873295\n",
      "2019-06-18 14:39:25,649 epoch 30 - iter 8/10 - loss 86.76356422\n",
      "2019-06-18 14:39:29,272 epoch 30 - iter 9/10 - loss 86.14871674\n",
      "2019-06-18 14:39:29,864 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:39:29,866 EPOCH 30 done: loss 86.1487 - lr 0.0008 - bad epochs 0\n",
      "2019-06-18 14:39:42,760 DEV : loss 101.15911865234375 - score 0.9226\n",
      "2019-06-18 14:40:08,813 TEST : loss 84.06092834472656 - score 0.911\n",
      "2019-06-18 14:40:13,154 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:40:21,589 epoch 31 - iter 0/10 - loss 98.35775757\n",
      "2019-06-18 14:40:28,521 epoch 31 - iter 1/10 - loss 94.62134552\n",
      "2019-06-18 14:40:34,170 epoch 31 - iter 2/10 - loss 91.95172628\n",
      "2019-06-18 14:40:38,766 epoch 31 - iter 3/10 - loss 88.96779633\n",
      "2019-06-18 14:40:43,046 epoch 31 - iter 4/10 - loss 87.76068420\n",
      "2019-06-18 14:40:49,334 epoch 31 - iter 5/10 - loss 86.80477651\n",
      "2019-06-18 14:40:53,102 epoch 31 - iter 6/10 - loss 86.53769575\n",
      "2019-06-18 14:40:57,657 epoch 31 - iter 7/10 - loss 86.35206509\n",
      "2019-06-18 14:41:04,462 epoch 31 - iter 8/10 - loss 85.38271332\n",
      "2019-06-18 14:41:09,019 epoch 31 - iter 9/10 - loss 86.25231323\n",
      "2019-06-18 14:41:09,653 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:41:09,657 EPOCH 31 done: loss 86.2523 - lr 0.0008 - bad epochs 1\n",
      "2019-06-18 14:41:22,570 DEV : loss 101.10504150390625 - score 0.9226\n",
      "2019-06-18 14:41:48,624 TEST : loss 84.10682678222656 - score 0.911\n",
      "2019-06-18 14:41:52,934 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:42:01,136 epoch 32 - iter 0/10 - loss 94.79573059\n",
      "2019-06-18 14:42:06,269 epoch 32 - iter 1/10 - loss 92.74620056\n",
      "2019-06-18 14:42:12,819 epoch 32 - iter 2/10 - loss 92.30054728\n",
      "2019-06-18 14:42:19,459 epoch 32 - iter 3/10 - loss 92.96216583\n",
      "2019-06-18 14:42:23,949 epoch 32 - iter 4/10 - loss 90.88872070\n",
      "2019-06-18 14:42:28,297 epoch 32 - iter 5/10 - loss 86.82557170\n",
      "2019-06-18 14:42:34,512 epoch 32 - iter 6/10 - loss 87.25437055\n",
      "2019-06-18 14:42:40,025 epoch 32 - iter 7/10 - loss 87.37718391\n",
      "2019-06-18 14:42:44,333 epoch 32 - iter 8/10 - loss 86.50353411\n",
      "2019-06-18 14:42:48,701 epoch 32 - iter 9/10 - loss 86.15587387\n",
      "2019-06-18 14:42:49,303 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:42:49,305 EPOCH 32 done: loss 86.1559 - lr 0.0008 - bad epochs 2\n",
      "2019-06-18 14:43:02,008 DEV : loss 101.08067321777344 - score 0.9226\n",
      "2019-06-18 14:43:27,856 TEST : loss 84.10225677490234 - score 0.911\n",
      "2019-06-18 14:43:32,171 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:43:40,380 epoch 33 - iter 0/10 - loss 82.97905731\n",
      "2019-06-18 14:43:46,095 epoch 33 - iter 1/10 - loss 79.25094986\n",
      "2019-06-18 14:43:53,006 epoch 33 - iter 2/10 - loss 85.25509644\n",
      "2019-06-18 14:43:57,891 epoch 33 - iter 3/10 - loss 87.12211800\n",
      "2019-06-18 14:44:02,208 epoch 33 - iter 4/10 - loss 87.41411285\n",
      "2019-06-18 14:44:06,807 epoch 33 - iter 5/10 - loss 86.23013941\n",
      "2019-06-18 14:44:09,927 epoch 33 - iter 6/10 - loss 84.33300018\n",
      "2019-06-18 14:44:14,919 epoch 33 - iter 7/10 - loss 87.39307404\n",
      "2019-06-18 14:44:18,951 epoch 33 - iter 8/10 - loss 86.30187649\n",
      "2019-06-18 14:44:24,154 epoch 33 - iter 9/10 - loss 86.10942764\n",
      "2019-06-18 14:44:24,763 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:44:24,765 EPOCH 33 done: loss 86.1094 - lr 0.0008 - bad epochs 3\n",
      "2019-06-18 14:44:37,375 DEV : loss 101.03584289550781 - score 0.9226\n",
      "2019-06-18 14:45:02,906 TEST : loss 84.05313110351562 - score 0.911\n",
      "Epoch    32: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2019-06-18 14:45:07,217 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:45:16,135 epoch 34 - iter 0/10 - loss 89.76803589\n",
      "2019-06-18 14:45:20,090 epoch 34 - iter 1/10 - loss 89.37900543\n",
      "2019-06-18 14:45:24,500 epoch 34 - iter 2/10 - loss 83.43105062\n",
      "2019-06-18 14:45:28,951 epoch 34 - iter 3/10 - loss 82.76505089\n",
      "2019-06-18 14:45:35,371 epoch 34 - iter 4/10 - loss 83.05118866\n",
      "2019-06-18 14:45:38,681 epoch 34 - iter 5/10 - loss 84.55185827\n",
      "2019-06-18 14:45:45,076 epoch 34 - iter 6/10 - loss 85.96440887\n",
      "2019-06-18 14:45:49,972 epoch 34 - iter 7/10 - loss 85.36521626\n",
      "2019-06-18 14:45:57,644 epoch 34 - iter 8/10 - loss 86.28667874\n",
      "2019-06-18 14:46:03,016 epoch 34 - iter 9/10 - loss 86.10788040\n",
      "2019-06-18 14:46:03,630 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:46:03,633 EPOCH 34 done: loss 86.1079 - lr 0.0004 - bad epochs 0\n",
      "2019-06-18 14:46:14,743 DEV : loss 101.06005096435547 - score 0.9226\n",
      "2019-06-18 14:46:40,704 TEST : loss 84.03340911865234 - score 0.911\n",
      "2019-06-18 14:46:45,059 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:46:50,306 epoch 35 - iter 0/10 - loss 75.32382202\n",
      "2019-06-18 14:46:57,299 epoch 35 - iter 1/10 - loss 79.94853973\n",
      "2019-06-18 14:47:01,952 epoch 35 - iter 2/10 - loss 80.15871938\n",
      "2019-06-18 14:47:08,760 epoch 35 - iter 3/10 - loss 81.89218521\n",
      "2019-06-18 14:47:14,273 epoch 35 - iter 4/10 - loss 84.22299500\n",
      "2019-06-18 14:47:19,107 epoch 35 - iter 5/10 - loss 87.21987534\n",
      "2019-06-18 14:47:23,666 epoch 35 - iter 6/10 - loss 86.43468693\n",
      "2019-06-18 14:47:30,101 epoch 35 - iter 7/10 - loss 87.07081699\n",
      "2019-06-18 14:47:37,316 epoch 35 - iter 8/10 - loss 87.02592044\n",
      "2019-06-18 14:47:41,631 epoch 35 - iter 9/10 - loss 86.10458374\n",
      "2019-06-18 14:47:42,256 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:47:42,257 EPOCH 35 done: loss 86.1046 - lr 0.0004 - bad epochs 1\n",
      "2019-06-18 14:47:53,263 DEV : loss 101.02188110351562 - score 0.9226\n",
      "2019-06-18 14:48:18,899 TEST : loss 84.01570129394531 - score 0.911\n",
      "2019-06-18 14:48:23,204 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:48:33,440 epoch 36 - iter 0/10 - loss 101.50713348\n",
      "2019-06-18 14:48:38,055 epoch 36 - iter 1/10 - loss 89.90908813\n",
      "2019-06-18 14:48:42,680 epoch 36 - iter 2/10 - loss 86.99278514\n",
      "2019-06-18 14:48:47,368 epoch 36 - iter 3/10 - loss 85.83440018\n",
      "2019-06-18 14:48:51,848 epoch 36 - iter 4/10 - loss 84.14351044\n",
      "2019-06-18 14:48:58,676 epoch 36 - iter 5/10 - loss 87.14075089\n",
      "2019-06-18 14:49:03,621 epoch 36 - iter 6/10 - loss 88.87017277\n",
      "2019-06-18 14:49:06,967 epoch 36 - iter 7/10 - loss 87.38749599\n",
      "2019-06-18 14:49:15,507 epoch 36 - iter 8/10 - loss 86.26378377\n",
      "2019-06-18 14:49:19,246 epoch 36 - iter 9/10 - loss 86.13309937\n",
      "2019-06-18 14:49:19,876 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:49:19,882 EPOCH 36 done: loss 86.1331 - lr 0.0004 - bad epochs 2\n",
      "2019-06-18 14:49:30,936 DEV : loss 101.02456665039062 - score 0.9226\n",
      "2019-06-18 14:49:57,433 TEST : loss 83.98597717285156 - score 0.911\n",
      "2019-06-18 14:50:01,772 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:50:07,719 epoch 37 - iter 0/10 - loss 75.60084534\n",
      "2019-06-18 14:50:16,570 epoch 37 - iter 1/10 - loss 89.24499512\n",
      "2019-06-18 14:50:20,837 epoch 37 - iter 2/10 - loss 84.62970225\n",
      "2019-06-18 14:50:27,765 epoch 37 - iter 3/10 - loss 88.93839264\n",
      "2019-06-18 14:50:32,341 epoch 37 - iter 4/10 - loss 85.88020630\n",
      "2019-06-18 14:50:38,455 epoch 37 - iter 5/10 - loss 86.96550242\n",
      "2019-06-18 14:50:42,040 epoch 37 - iter 6/10 - loss 86.46033587\n",
      "2019-06-18 14:50:45,807 epoch 37 - iter 7/10 - loss 85.63767338\n",
      "2019-06-18 14:50:49,782 epoch 37 - iter 8/10 - loss 85.83535343\n",
      "2019-06-18 14:50:54,506 epoch 37 - iter 9/10 - loss 86.18360367\n",
      "2019-06-18 14:50:55,120 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:50:55,126 EPOCH 37 done: loss 86.1836 - lr 0.0004 - bad epochs 3\n",
      "2019-06-18 14:51:07,756 DEV : loss 100.97862243652344 - score 0.9226\n",
      "2019-06-18 14:51:33,583 TEST : loss 83.98237609863281 - score 0.911\n",
      "Epoch    36: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2019-06-18 14:51:38,024 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:51:44,486 epoch 38 - iter 0/10 - loss 79.65013885\n",
      "2019-06-18 14:51:51,267 epoch 38 - iter 1/10 - loss 82.78049850\n",
      "2019-06-18 14:51:57,728 epoch 38 - iter 2/10 - loss 84.35241191\n",
      "2019-06-18 14:52:02,621 epoch 38 - iter 3/10 - loss 89.41783142\n",
      "2019-06-18 14:52:07,319 epoch 38 - iter 4/10 - loss 89.17865295\n",
      "2019-06-18 14:52:12,801 epoch 38 - iter 5/10 - loss 89.12897746\n",
      "2019-06-18 14:52:19,757 epoch 38 - iter 6/10 - loss 88.56628418\n",
      "2019-06-18 14:52:22,963 epoch 38 - iter 7/10 - loss 86.92081070\n",
      "2019-06-18 14:52:27,609 epoch 38 - iter 8/10 - loss 86.58159044\n",
      "2019-06-18 14:52:34,001 epoch 38 - iter 9/10 - loss 86.13276520\n",
      "2019-06-18 14:52:34,617 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:52:34,619 EPOCH 38 done: loss 86.1328 - lr 0.0002 - bad epochs 0\n",
      "2019-06-18 14:52:47,108 DEV : loss 101.03327941894531 - score 0.9226\n",
      "2019-06-18 14:53:13,025 TEST : loss 83.98767852783203 - score 0.911\n",
      "2019-06-18 14:53:17,383 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:53:24,144 epoch 39 - iter 0/10 - loss 86.15620422\n",
      "2019-06-18 14:53:30,771 epoch 39 - iter 1/10 - loss 88.22941971\n",
      "2019-06-18 14:53:35,485 epoch 39 - iter 2/10 - loss 85.97391001\n",
      "2019-06-18 14:53:39,395 epoch 39 - iter 3/10 - loss 84.94108963\n",
      "2019-06-18 14:53:47,377 epoch 39 - iter 4/10 - loss 85.59222717\n",
      "2019-06-18 14:53:52,904 epoch 39 - iter 5/10 - loss 86.52135213\n",
      "2019-06-18 14:53:57,083 epoch 39 - iter 6/10 - loss 84.57014901\n",
      "2019-06-18 14:54:03,847 epoch 39 - iter 7/10 - loss 87.19821548\n",
      "2019-06-18 14:54:08,717 epoch 39 - iter 8/10 - loss 86.87695821\n",
      "2019-06-18 14:54:13,886 epoch 39 - iter 9/10 - loss 85.95601349\n",
      "2019-06-18 14:54:14,496 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:54:14,499 EPOCH 39 done: loss 85.9560 - lr 0.0002 - bad epochs 1\n",
      "2019-06-18 14:54:25,523 DEV : loss 100.97456359863281 - score 0.9226\n",
      "2019-06-18 14:54:51,096 TEST : loss 83.9809341430664 - score 0.911\n",
      "2019-06-18 14:54:55,396 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:55:01,819 epoch 40 - iter 0/10 - loss 91.50212097\n",
      "2019-06-18 14:55:06,679 epoch 40 - iter 1/10 - loss 89.59986877\n",
      "2019-06-18 14:55:11,593 epoch 40 - iter 2/10 - loss 88.27263641\n",
      "2019-06-18 14:55:15,893 epoch 40 - iter 3/10 - loss 86.08429527\n",
      "2019-06-18 14:55:19,629 epoch 40 - iter 4/10 - loss 82.89636230\n",
      "2019-06-18 14:55:25,027 epoch 40 - iter 5/10 - loss 83.08727392\n",
      "2019-06-18 14:55:32,052 epoch 40 - iter 6/10 - loss 84.44862366\n",
      "2019-06-18 14:55:37,603 epoch 40 - iter 7/10 - loss 85.97105789\n",
      "2019-06-18 14:55:42,846 epoch 40 - iter 8/10 - loss 85.96227943\n",
      "2019-06-18 14:55:49,101 epoch 40 - iter 9/10 - loss 86.07707748\n",
      "2019-06-18 14:55:49,700 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:55:49,702 EPOCH 40 done: loss 86.0771 - lr 0.0002 - bad epochs 2\n",
      "2019-06-18 14:56:00,740 DEV : loss 100.93692016601562 - score 0.9226\n",
      "2019-06-18 14:56:26,609 TEST : loss 83.95841979980469 - score 0.911\n",
      "2019-06-18 14:56:30,948 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:56:39,819 epoch 41 - iter 0/10 - loss 83.59997559\n",
      "2019-06-18 14:56:46,451 epoch 41 - iter 1/10 - loss 86.32300568\n",
      "2019-06-18 14:56:52,058 epoch 41 - iter 2/10 - loss 90.22084045\n",
      "2019-06-18 14:56:55,835 epoch 41 - iter 3/10 - loss 86.64699364\n",
      "2019-06-18 14:57:00,453 epoch 41 - iter 4/10 - loss 85.91684113\n",
      "2019-06-18 14:57:06,756 epoch 41 - iter 5/10 - loss 84.50195948\n",
      "2019-06-18 14:57:10,036 epoch 41 - iter 6/10 - loss 84.19157410\n",
      "2019-06-18 14:57:14,891 epoch 41 - iter 7/10 - loss 85.74316597\n",
      "2019-06-18 14:57:23,340 epoch 41 - iter 8/10 - loss 85.31825426\n",
      "2019-06-18 14:57:27,951 epoch 41 - iter 9/10 - loss 86.19445419\n",
      "2019-06-18 14:57:28,579 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:57:28,580 EPOCH 41 done: loss 86.1945 - lr 0.0002 - bad epochs 3\n",
      "2019-06-18 14:57:39,630 DEV : loss 100.95906066894531 - score 0.9226\n",
      "2019-06-18 14:58:05,334 TEST : loss 83.95891571044922 - score 0.911\n",
      "Epoch    40: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2019-06-18 14:58:09,696 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:58:09,699 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:58:09,700 learning rate too small - quitting training!\n",
      "2019-06-18 14:58:09,702 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:58:13,859 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 14:58:13,865 Testing using best model ...\n",
      "2019-06-18 14:58:13,868 loading file resources/taggers/resume-ner-4/best-model.pt\n",
      "2019-06-18 14:58:42,679 0.911\t0.911\t0.911\n",
      "2019-06-18 14:58:42,681 \n",
      "MICRO_AVG: acc 0.8365 - f1-score 0.911\n",
      "MACRO_AVG: acc 0.1822 - f1-score 0.19068000000000002\n",
      "-\"         tp: 0 - fp: 0 - fn: 1366 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "Companies  tp: 0 - fp: 0 - fn: 778 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "Degree\"    tp: 0 - fp: 0 - fn: 353 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "O\"         tp: 41302 - fp: 4035 - fn: 0 - tn: 41302 - precision: 0.9110 - recall: 1.0000 - accuracy: 0.9110 - f1-score: 0.9534\n",
      "Skills\"    tp: 0 - fp: 0 - fn: 1538 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2019-06-18 14:58:42,685 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [tensor(219.5380, device='cuda:0'),\n",
       "  tensor(174.8539, device='cuda:0'),\n",
       "  tensor(154.1482, device='cuda:0'),\n",
       "  tensor(142.6956, device='cuda:0'),\n",
       "  tensor(135.3738, device='cuda:0'),\n",
       "  tensor(119.5223, device='cuda:0'),\n",
       "  tensor(115.7494, device='cuda:0'),\n",
       "  tensor(112.4148, device='cuda:0'),\n",
       "  tensor(111.3802, device='cuda:0'),\n",
       "  tensor(108.5417, device='cuda:0'),\n",
       "  tensor(107.6384, device='cuda:0'),\n",
       "  tensor(106.3457, device='cuda:0'),\n",
       "  tensor(106.2371, device='cuda:0'),\n",
       "  tensor(104.3350, device='cuda:0'),\n",
       "  tensor(103.4868, device='cuda:0'),\n",
       "  tensor(103.7613, device='cuda:0'),\n",
       "  tensor(103.6114, device='cuda:0'),\n",
       "  tensor(102.4051, device='cuda:0'),\n",
       "  tensor(102.4545, device='cuda:0'),\n",
       "  tensor(101.9461, device='cuda:0'),\n",
       "  tensor(101.8941, device='cuda:0'),\n",
       "  tensor(101.5799, device='cuda:0'),\n",
       "  tensor(101.5165, device='cuda:0'),\n",
       "  tensor(101.4032, device='cuda:0'),\n",
       "  tensor(101.3834, device='cuda:0'),\n",
       "  tensor(101.2200, device='cuda:0'),\n",
       "  tensor(101.1980, device='cuda:0'),\n",
       "  tensor(101.2397, device='cuda:0'),\n",
       "  tensor(101.2478, device='cuda:0'),\n",
       "  tensor(101.1591, device='cuda:0'),\n",
       "  tensor(101.1050, device='cuda:0'),\n",
       "  tensor(101.0807, device='cuda:0'),\n",
       "  tensor(101.0358, device='cuda:0'),\n",
       "  tensor(101.0601, device='cuda:0'),\n",
       "  tensor(101.0219, device='cuda:0'),\n",
       "  tensor(101.0246, device='cuda:0'),\n",
       "  tensor(100.9786, device='cuda:0'),\n",
       "  tensor(101.0333, device='cuda:0'),\n",
       "  tensor(100.9746, device='cuda:0'),\n",
       "  tensor(100.9369, device='cuda:0'),\n",
       "  tensor(100.9591, device='cuda:0')],\n",
       " 'dev_score_history': [0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226,\n",
       "  0.9226],\n",
       " 'test_score': 0.911,\n",
       " 'train_loss_history': [782.6478408813476,\n",
       "  172.02225189208986,\n",
       "  139.569669342041,\n",
       "  126.18888778686524,\n",
       "  116.74652633666992,\n",
       "  104.90873565673829,\n",
       "  101.5046012878418,\n",
       "  99.19527893066406,\n",
       "  97.04779434204102,\n",
       "  93.21143188476563,\n",
       "  92.36696701049804,\n",
       "  91.59069366455078,\n",
       "  90.9760124206543,\n",
       "  89.41272201538087,\n",
       "  89.19360580444337,\n",
       "  88.61053466796875,\n",
       "  87.8644401550293,\n",
       "  87.73026275634766,\n",
       "  87.24821319580079,\n",
       "  87.11863250732422,\n",
       "  87.3237091064453,\n",
       "  86.81343231201171,\n",
       "  86.95383987426757,\n",
       "  86.45633850097656,\n",
       "  86.34131088256837,\n",
       "  86.15289154052735,\n",
       "  86.30439605712891,\n",
       "  86.23973159790039,\n",
       "  86.3353645324707,\n",
       "  86.14871673583984,\n",
       "  86.25231323242187,\n",
       "  86.1558738708496,\n",
       "  86.10942764282227,\n",
       "  86.10788040161133,\n",
       "  86.10458374023438,\n",
       "  86.13309936523437,\n",
       "  86.1836036682129,\n",
       "  86.13276519775391,\n",
       "  85.95601348876953,\n",
       "  86.07707748413085,\n",
       "  86.19445419311523]}"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "## give your model a name and folder of your choice. Your model will be saved there for loading later \n",
    "## you can run this notebook many times with different embeddings/params and save the models with different names\n",
    "model_name = 'resources/taggers/resume-ner-4'\n",
    "\n",
    "# 7. start training - you can experiment with batch size if you get memory errors\n",
    "# how many epochs does it take before the model stops showing improvement? Start with a big number like 150, and stop the code cell\n",
    "# from running at any time - the framework will persist the best model even if you interrupt training. \n",
    "trainer.train(model_name,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              #anneal_with_restarts=True,\n",
    "              max_epochs=150)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "flair_nlp_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
