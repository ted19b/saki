{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flair_nlp_colab.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzcl9cimSGsL",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ted19b/saki_ss19/blob/oss-saki-ss19-exercice-2/uebung_02/flair_nlp_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAHTYJM1Rv5t",
        "colab_type": "text"
      },
      "source": [
        "**Resume NER Part 4: Working with Flair NLP**\n",
        "\n",
        "---\n",
        "\n",
        "In this part we will use flair NLP to train a model on our data and evaluate the results. Please make sure you have set up your Google account and uploaded your files to Google drive. This Notebook should run on Google Colab. Let's change the working directory to the Google drive where our training data is, and install flair nlp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE2aZnLqxvSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2oWWC1H0LMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Saki_2019/data/flair\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVCjf7mO0UNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download flair library #\n",
        "! pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTBAKPlNzlb7",
        "colab_type": "code",
        "outputId": "33ef89ef-6ace-4b3b-9944-d16e1d10c4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# imports \n",
        "from flair.datasets import Corpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "\n",
        "## make sure this describes your file structure\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "# folder where training and test data are\n",
        "data_folder = '/content/gdrive/My Drive/Saki_2019/data/flair'\n",
        "\n",
        "# 1.0 is full data, try a much smaller number like 0.1 to test run the code\n",
        "downsample = 1 \n",
        "\n",
        "## your train file name\n",
        "train_file = 'train_res_bilou.txt'\n",
        "\n",
        "## your test file name\n",
        "test_file = 'test_res_bilou.txt'\n",
        "\n",
        "# 1. get the corpus\n",
        "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, train_file=train_file, test_file=test_file, dev_file=None).downsample(downsample)\n",
        "print(corpus)\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
        "print(tag_dictionary.idx2item)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-18 16:31:56,408 Reading data from /content/gdrive/My Drive/Saki_2019/data/flair\n",
            "2019-06-18 16:31:56,410 Train: /content/gdrive/My Drive/Saki_2019/data/flair/train_res_bilou.txt\n",
            "2019-06-18 16:31:56,411 Dev: None\n",
            "2019-06-18 16:31:56,420 Test: /content/gdrive/My Drive/Saki_2019/data/flair/test_res_bilou.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:312: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  train_file, column_format\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:318: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  test_file, column_format\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Corpus: 167449 train + 18606 dev + 48803 test sentences\n",
            "[b'<unk>', b'O', b'Skills', b'-', b'Degree', b'Companies', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBgwsFQR2HTC",
        "colab_type": "code",
        "outputId": "ac66860a-a212-48b3-fbe9-278d7dd74c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from typing import List\n",
        "\n",
        "# 4. initialize embeddings. Experiment with different embedding types to see what gets the best results\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings,FlairEmbeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "    WordEmbeddings('glove'),\n",
        "    # comment in this line to use character embeddings\n",
        "    # CharacterEmbeddings(),\n",
        "\n",
        "    # comment in these lines to use flair embeddings (needs a LONG time to train :-)\n",
        "    #FlairEmbeddings('news-forward'),\n",
        "    #FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type='ner',\n",
        "                                        use_crf=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmQOUUy52TQ6",
        "colab_type": "code",
        "outputId": "846ff78c-d8ee-4538-d7fd-472cdd2139ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10368
        }
      },
      "source": [
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "## give your model a name and folder of your choice. Your model will be saved there for loading later \n",
        "## you can run this notebook many times with different embeddings/params and save the models with different names\n",
        "model_name = 'resources/taggers/resume-ner'\n",
        "\n",
        "# 7. start training - you can experiment with batch size if you get memory errors\n",
        "# how many epochs does it take before the model stops showing improvement? Start with a big number like 150, and stop the code cell\n",
        "# from running at any time - the framework will persist the best model even if you interrupt training. \n",
        "trainer.train(model_name,\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              #anneal_with_restarts=True,\n",
        "              max_epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-18 16:34:25,410 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:34:25,412 Evaluation method: MICRO_F1_SCORE\n",
            "2019-06-18 16:34:25,739 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:34:26,196 epoch 1 - iter 0/5233 - loss 2.14313650\n",
            "2019-06-18 16:35:02,066 epoch 1 - iter 523/5233 - loss 0.40119791\n",
            "2019-06-18 16:35:38,095 epoch 1 - iter 1046/5233 - loss 0.37913694\n",
            "2019-06-18 16:36:13,841 epoch 1 - iter 1569/5233 - loss 0.37753708\n",
            "2019-06-18 16:36:49,658 epoch 1 - iter 2092/5233 - loss 0.37006825\n",
            "2019-06-18 16:37:25,528 epoch 1 - iter 2615/5233 - loss 0.36868961\n",
            "2019-06-18 16:38:01,291 epoch 1 - iter 3138/5233 - loss 0.36435227\n",
            "2019-06-18 16:38:37,123 epoch 1 - iter 3661/5233 - loss 0.36310076\n",
            "2019-06-18 16:39:13,012 epoch 1 - iter 4184/5233 - loss 0.35998051\n",
            "2019-06-18 16:39:48,741 epoch 1 - iter 4707/5233 - loss 0.35746186\n",
            "2019-06-18 16:40:24,503 epoch 1 - iter 5230/5233 - loss 0.35585041\n",
            "2019-06-18 16:40:24,870 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:40:24,874 EPOCH 1 done: loss 0.3559 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 16:40:52,027 DEV : loss 0.3288666605949402 - score 0.0202\n",
            "2019-06-18 16:42:02,048 TEST : loss 0.30951425433158875 - score 0.0108\n",
            "2019-06-18 16:42:06,167 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:42:06,747 epoch 2 - iter 0/5233 - loss 0.60896009\n",
            "2019-06-18 16:42:43,160 epoch 2 - iter 523/5233 - loss 0.35184281\n",
            "2019-06-18 16:43:19,546 epoch 2 - iter 1046/5233 - loss 0.34863194\n",
            "2019-06-18 16:43:57,307 epoch 2 - iter 1569/5233 - loss 0.34337266\n",
            "2019-06-18 16:44:33,572 epoch 2 - iter 2092/5233 - loss 0.33692152\n",
            "2019-06-18 16:45:09,831 epoch 2 - iter 2615/5233 - loss 0.33876655\n",
            "2019-06-18 16:45:46,072 epoch 2 - iter 3138/5233 - loss 0.34048827\n",
            "2019-06-18 16:46:22,375 epoch 2 - iter 3661/5233 - loss 0.34034025\n",
            "2019-06-18 16:46:58,512 epoch 2 - iter 4184/5233 - loss 0.34013299\n",
            "2019-06-18 16:47:34,736 epoch 2 - iter 4707/5233 - loss 0.33926192\n",
            "2019-06-18 16:48:10,892 epoch 2 - iter 5230/5233 - loss 0.33904994\n",
            "2019-06-18 16:48:11,286 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:48:11,287 EPOCH 2 done: loss 0.3392 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 16:48:38,747 DEV : loss 0.3258601129055023 - score 0.0533\n",
            "2019-06-18 16:49:49,616 TEST : loss 0.307539701461792 - score 0.0413\n",
            "2019-06-18 16:49:53,693 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:49:54,316 epoch 3 - iter 0/5233 - loss 0.55677414\n",
            "2019-06-18 16:50:30,741 epoch 3 - iter 523/5233 - loss 0.33385733\n",
            "2019-06-18 16:51:07,172 epoch 3 - iter 1046/5233 - loss 0.33447182\n",
            "2019-06-18 16:51:43,463 epoch 3 - iter 1569/5233 - loss 0.33696597\n",
            "2019-06-18 16:52:19,668 epoch 3 - iter 2092/5233 - loss 0.33609546\n",
            "2019-06-18 16:52:55,796 epoch 3 - iter 2615/5233 - loss 0.33838388\n",
            "2019-06-18 16:53:31,762 epoch 3 - iter 3138/5233 - loss 0.33766299\n",
            "2019-06-18 16:54:07,501 epoch 3 - iter 3661/5233 - loss 0.33634373\n",
            "2019-06-18 16:54:43,253 epoch 3 - iter 4184/5233 - loss 0.33634059\n",
            "2019-06-18 16:55:19,180 epoch 3 - iter 4707/5233 - loss 0.33689373\n",
            "2019-06-18 16:55:55,047 epoch 3 - iter 5230/5233 - loss 0.33688546\n",
            "2019-06-18 16:55:55,456 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:55:55,457 EPOCH 3 done: loss 0.3368 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 16:56:22,800 DEV : loss 0.32130563259124756 - score 0.0887\n",
            "2019-06-18 16:57:33,163 TEST : loss 0.3018285632133484 - score 0.0967\n",
            "2019-06-18 16:57:37,186 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 16:57:37,812 epoch 4 - iter 0/5233 - loss 0.35610372\n",
            "2019-06-18 16:58:14,046 epoch 4 - iter 523/5233 - loss 0.33634335\n",
            "2019-06-18 16:58:50,254 epoch 4 - iter 1046/5233 - loss 0.33743960\n",
            "2019-06-18 16:59:26,301 epoch 4 - iter 1569/5233 - loss 0.33419749\n",
            "2019-06-18 17:00:02,236 epoch 4 - iter 2092/5233 - loss 0.33524521\n",
            "2019-06-18 17:00:38,299 epoch 4 - iter 2615/5233 - loss 0.33411161\n",
            "2019-06-18 17:01:14,377 epoch 4 - iter 3138/5233 - loss 0.33442124\n",
            "2019-06-18 17:01:50,390 epoch 4 - iter 3661/5233 - loss 0.33408461\n",
            "2019-06-18 17:02:26,428 epoch 4 - iter 4184/5233 - loss 0.33441024\n",
            "2019-06-18 17:03:02,427 epoch 4 - iter 4707/5233 - loss 0.33401441\n",
            "2019-06-18 17:03:38,427 epoch 4 - iter 5230/5233 - loss 0.33411834\n",
            "2019-06-18 17:03:38,835 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:03:38,837 EPOCH 4 done: loss 0.3341 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 17:04:06,064 DEV : loss 0.31906643509864807 - score 0.0653\n",
            "2019-06-18 17:05:16,429 TEST : loss 0.298452228307724 - score 0.0768\n",
            "2019-06-18 17:05:16,431 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:05:17,093 epoch 5 - iter 0/5233 - loss 0.45514134\n",
            "2019-06-18 17:05:53,279 epoch 5 - iter 523/5233 - loss 0.33652945\n",
            "2019-06-18 17:06:29,481 epoch 5 - iter 1046/5233 - loss 0.33647895\n",
            "2019-06-18 17:07:05,615 epoch 5 - iter 1569/5233 - loss 0.33524919\n",
            "2019-06-18 17:07:41,595 epoch 5 - iter 2092/5233 - loss 0.33471936\n",
            "2019-06-18 17:08:19,682 epoch 5 - iter 2615/5233 - loss 0.33129235\n",
            "2019-06-18 17:08:55,762 epoch 5 - iter 3138/5233 - loss 0.33087548\n",
            "2019-06-18 17:09:31,979 epoch 5 - iter 3661/5233 - loss 0.33034263\n",
            "2019-06-18 17:10:07,973 epoch 5 - iter 4184/5233 - loss 0.33078285\n",
            "2019-06-18 17:10:43,951 epoch 5 - iter 4707/5233 - loss 0.33102782\n",
            "2019-06-18 17:11:20,063 epoch 5 - iter 5230/5233 - loss 0.33114764\n",
            "2019-06-18 17:11:20,492 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:11:20,494 EPOCH 5 done: loss 0.3311 - lr 0.1000 - bad epochs 1\n",
            "2019-06-18 17:11:48,010 DEV : loss 0.3152942359447479 - score 0.0952\n",
            "2019-06-18 17:12:58,256 TEST : loss 0.29711511731147766 - score 0.1073\n",
            "2019-06-18 17:13:02,304 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:13:02,948 epoch 6 - iter 0/5233 - loss 0.24499488\n",
            "2019-06-18 17:13:38,990 epoch 6 - iter 523/5233 - loss 0.32731073\n",
            "2019-06-18 17:14:15,204 epoch 6 - iter 1046/5233 - loss 0.32594231\n",
            "2019-06-18 17:14:50,773 epoch 6 - iter 1569/5233 - loss 0.32931615\n",
            "2019-06-18 17:15:26,278 epoch 6 - iter 2092/5233 - loss 0.32912312\n",
            "2019-06-18 17:16:02,190 epoch 6 - iter 2615/5233 - loss 0.32764252\n",
            "2019-06-18 17:16:38,086 epoch 6 - iter 3138/5233 - loss 0.32526107\n",
            "2019-06-18 17:17:14,198 epoch 6 - iter 3661/5233 - loss 0.32678665\n",
            "2019-06-18 17:17:50,246 epoch 6 - iter 4184/5233 - loss 0.32725294\n",
            "2019-06-18 17:18:26,143 epoch 6 - iter 4707/5233 - loss 0.32790885\n",
            "2019-06-18 17:19:01,603 epoch 6 - iter 5230/5233 - loss 0.32806105\n",
            "2019-06-18 17:19:02,020 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:19:02,022 EPOCH 6 done: loss 0.3280 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 17:19:29,392 DEV : loss 0.31216591596603394 - score 0.1029\n",
            "2019-06-18 17:20:39,425 TEST : loss 0.2940259277820587 - score 0.1185\n",
            "2019-06-18 17:20:43,462 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:20:44,104 epoch 7 - iter 0/5233 - loss 0.25111616\n",
            "2019-06-18 17:21:20,560 epoch 7 - iter 523/5233 - loss 0.32715906\n",
            "2019-06-18 17:21:56,824 epoch 7 - iter 1046/5233 - loss 0.32905641\n",
            "2019-06-18 17:22:33,085 epoch 7 - iter 1569/5233 - loss 0.32643697\n",
            "2019-06-18 17:23:09,203 epoch 7 - iter 2092/5233 - loss 0.32896481\n",
            "2019-06-18 17:23:45,507 epoch 7 - iter 2615/5233 - loss 0.32776462\n",
            "2019-06-18 17:24:21,662 epoch 7 - iter 3138/5233 - loss 0.32660313\n",
            "2019-06-18 17:24:57,584 epoch 7 - iter 3661/5233 - loss 0.32608881\n",
            "2019-06-18 17:25:33,861 epoch 7 - iter 4184/5233 - loss 0.32731251\n",
            "2019-06-18 17:26:09,978 epoch 7 - iter 4707/5233 - loss 0.32609839\n",
            "2019-06-18 17:26:45,938 epoch 7 - iter 5230/5233 - loss 0.32675430\n",
            "2019-06-18 17:26:46,336 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:26:46,338 EPOCH 7 done: loss 0.3267 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 17:27:13,671 DEV : loss 0.3112987279891968 - score 0.0995\n",
            "2019-06-18 17:28:23,709 TEST : loss 0.291955828666687 - score 0.1144\n",
            "2019-06-18 17:28:23,715 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:28:24,362 epoch 8 - iter 0/5233 - loss 0.49724317\n",
            "2019-06-18 17:29:00,550 epoch 8 - iter 523/5233 - loss 0.32094618\n",
            "2019-06-18 17:29:36,964 epoch 8 - iter 1046/5233 - loss 0.32616627\n",
            "2019-06-18 17:30:13,092 epoch 8 - iter 1569/5233 - loss 0.32832793\n",
            "2019-06-18 17:30:49,038 epoch 8 - iter 2092/5233 - loss 0.32805780\n",
            "2019-06-18 17:31:25,122 epoch 8 - iter 2615/5233 - loss 0.32796627\n",
            "2019-06-18 17:32:01,196 epoch 8 - iter 3138/5233 - loss 0.32618675\n",
            "2019-06-18 17:32:39,228 epoch 8 - iter 3661/5233 - loss 0.32619449\n",
            "2019-06-18 17:33:15,405 epoch 8 - iter 4184/5233 - loss 0.32746207\n",
            "2019-06-18 17:33:51,690 epoch 8 - iter 4707/5233 - loss 0.32727457\n",
            "2019-06-18 17:34:27,915 epoch 8 - iter 5230/5233 - loss 0.32618312\n",
            "2019-06-18 17:34:28,353 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:34:28,354 EPOCH 8 done: loss 0.3262 - lr 0.1000 - bad epochs 1\n",
            "2019-06-18 17:34:55,774 DEV : loss 0.30803629755973816 - score 0.1014\n",
            "2019-06-18 17:36:06,783 TEST : loss 0.2916736900806427 - score 0.1184\n",
            "2019-06-18 17:36:06,786 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:36:07,423 epoch 9 - iter 0/5233 - loss 0.27474463\n",
            "2019-06-18 17:36:44,042 epoch 9 - iter 523/5233 - loss 0.32991198\n",
            "2019-06-18 17:37:20,631 epoch 9 - iter 1046/5233 - loss 0.32514579\n",
            "2019-06-18 17:37:57,446 epoch 9 - iter 1569/5233 - loss 0.32618393\n",
            "2019-06-18 17:38:33,772 epoch 9 - iter 2092/5233 - loss 0.32237043\n",
            "2019-06-18 17:39:10,147 epoch 9 - iter 2615/5233 - loss 0.32106339\n",
            "2019-06-18 17:39:46,633 epoch 9 - iter 3138/5233 - loss 0.32228193\n",
            "2019-06-18 17:40:22,930 epoch 9 - iter 3661/5233 - loss 0.32393161\n",
            "2019-06-18 17:40:59,202 epoch 9 - iter 4184/5233 - loss 0.32268797\n",
            "2019-06-18 17:41:35,640 epoch 9 - iter 4707/5233 - loss 0.32413026\n",
            "2019-06-18 17:42:12,000 epoch 9 - iter 5230/5233 - loss 0.32454376\n",
            "2019-06-18 17:42:12,400 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:42:12,402 EPOCH 9 done: loss 0.3245 - lr 0.1000 - bad epochs 2\n",
            "2019-06-18 17:42:39,817 DEV : loss 0.3105562627315521 - score 0.1107\n",
            "2019-06-18 17:43:51,085 TEST : loss 0.2918439209461212 - score 0.1263\n",
            "2019-06-18 17:43:55,187 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:43:55,817 epoch 10 - iter 0/5233 - loss 0.26003867\n",
            "2019-06-18 17:44:32,385 epoch 10 - iter 523/5233 - loss 0.31746828\n",
            "2019-06-18 17:45:08,835 epoch 10 - iter 1046/5233 - loss 0.32082085\n",
            "2019-06-18 17:45:45,189 epoch 10 - iter 1569/5233 - loss 0.32520601\n",
            "2019-06-18 17:46:21,471 epoch 10 - iter 2092/5233 - loss 0.32402054\n",
            "2019-06-18 17:46:57,717 epoch 10 - iter 2615/5233 - loss 0.32375016\n",
            "2019-06-18 17:47:34,056 epoch 10 - iter 3138/5233 - loss 0.32365803\n",
            "2019-06-18 17:48:10,512 epoch 10 - iter 3661/5233 - loss 0.32451264\n",
            "2019-06-18 17:48:46,849 epoch 10 - iter 4184/5233 - loss 0.32357611\n",
            "2019-06-18 17:49:23,272 epoch 10 - iter 4707/5233 - loss 0.32485581\n",
            "2019-06-18 17:49:59,622 epoch 10 - iter 5230/5233 - loss 0.32375566\n",
            "2019-06-18 17:50:00,024 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:50:00,026 EPOCH 10 done: loss 0.3237 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 17:50:27,364 DEV : loss 0.3121083974838257 - score 0.0942\n",
            "2019-06-18 17:51:37,920 TEST : loss 0.2903093993663788 - score 0.1139\n",
            "2019-06-18 17:51:37,923 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:51:38,567 epoch 11 - iter 0/5233 - loss 0.17160308\n",
            "2019-06-18 17:52:15,206 epoch 11 - iter 523/5233 - loss 0.32417736\n",
            "2019-06-18 17:52:51,702 epoch 11 - iter 1046/5233 - loss 0.32794657\n",
            "2019-06-18 17:53:28,251 epoch 11 - iter 1569/5233 - loss 0.33139806\n",
            "2019-06-18 17:54:04,449 epoch 11 - iter 2092/5233 - loss 0.32610168\n",
            "2019-06-18 17:54:40,566 epoch 11 - iter 2615/5233 - loss 0.32347555\n",
            "2019-06-18 17:55:16,696 epoch 11 - iter 3138/5233 - loss 0.32016599\n",
            "2019-06-18 17:55:52,717 epoch 11 - iter 3661/5233 - loss 0.32196155\n",
            "2019-06-18 17:56:28,732 epoch 11 - iter 4184/5233 - loss 0.32043652\n",
            "2019-06-18 17:57:06,796 epoch 11 - iter 4707/5233 - loss 0.32144450\n",
            "2019-06-18 17:57:43,216 epoch 11 - iter 5230/5233 - loss 0.32183723\n",
            "2019-06-18 17:57:43,637 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:57:43,639 EPOCH 11 done: loss 0.3218 - lr 0.1000 - bad epochs 1\n",
            "2019-06-18 17:58:11,047 DEV : loss 0.30651700496673584 - score 0.1161\n",
            "2019-06-18 17:59:22,324 TEST : loss 0.28820711374282837 - score 0.1322\n",
            "2019-06-18 17:59:26,401 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 17:59:27,033 epoch 12 - iter 0/5233 - loss 0.22554687\n",
            "2019-06-18 18:00:03,603 epoch 12 - iter 523/5233 - loss 0.30625593\n",
            "2019-06-18 18:00:39,907 epoch 12 - iter 1046/5233 - loss 0.31442228\n",
            "2019-06-18 18:01:16,224 epoch 12 - iter 1569/5233 - loss 0.31847427\n",
            "2019-06-18 18:01:52,234 epoch 12 - iter 2092/5233 - loss 0.32023907\n",
            "2019-06-18 18:02:28,150 epoch 12 - iter 2615/5233 - loss 0.32084376\n",
            "2019-06-18 18:03:04,321 epoch 12 - iter 3138/5233 - loss 0.32062150\n",
            "2019-06-18 18:03:40,625 epoch 12 - iter 3661/5233 - loss 0.32109893\n",
            "2019-06-18 18:04:16,493 epoch 12 - iter 4184/5233 - loss 0.32271248\n",
            "2019-06-18 18:04:52,373 epoch 12 - iter 4707/5233 - loss 0.32255063\n",
            "2019-06-18 18:05:28,050 epoch 12 - iter 5230/5233 - loss 0.32122523\n",
            "2019-06-18 18:05:28,450 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:05:28,452 EPOCH 12 done: loss 0.3211 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 18:05:55,515 DEV : loss 0.30783364176750183 - score 0.1389\n",
            "2019-06-18 18:07:05,510 TEST : loss 0.2889956831932068 - score 0.1471\n",
            "2019-06-18 18:07:09,556 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:07:10,196 epoch 13 - iter 0/5233 - loss 0.27501673\n",
            "2019-06-18 18:07:46,129 epoch 13 - iter 523/5233 - loss 0.31900034\n",
            "2019-06-18 18:08:21,821 epoch 13 - iter 1046/5233 - loss 0.31666188\n",
            "2019-06-18 18:08:57,990 epoch 13 - iter 1569/5233 - loss 0.31804003\n",
            "2019-06-18 18:09:33,882 epoch 13 - iter 2092/5233 - loss 0.31940916\n",
            "2019-06-18 18:10:09,885 epoch 13 - iter 2615/5233 - loss 0.31978187\n",
            "2019-06-18 18:10:46,268 epoch 13 - iter 3138/5233 - loss 0.31880224\n",
            "2019-06-18 18:11:21,943 epoch 13 - iter 3661/5233 - loss 0.32049441\n",
            "2019-06-18 18:11:57,767 epoch 13 - iter 4184/5233 - loss 0.32156944\n",
            "2019-06-18 18:12:34,018 epoch 13 - iter 4707/5233 - loss 0.32149891\n",
            "2019-06-18 18:13:10,230 epoch 13 - iter 5230/5233 - loss 0.32196854\n",
            "2019-06-18 18:13:10,639 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:13:10,644 EPOCH 13 done: loss 0.3219 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 18:13:37,978 DEV : loss 0.3042183518409729 - score 0.1428\n",
            "2019-06-18 18:14:48,319 TEST : loss 0.2864026427268982 - score 0.1494\n",
            "2019-06-18 18:14:52,370 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:14:53,013 epoch 14 - iter 0/5233 - loss 0.53526974\n",
            "2019-06-18 18:15:29,609 epoch 14 - iter 523/5233 - loss 0.31922766\n",
            "2019-06-18 18:16:05,919 epoch 14 - iter 1046/5233 - loss 0.31999650\n",
            "2019-06-18 18:16:42,117 epoch 14 - iter 1569/5233 - loss 0.32112811\n",
            "2019-06-18 18:17:18,498 epoch 14 - iter 2092/5233 - loss 0.31657832\n",
            "2019-06-18 18:17:54,848 epoch 14 - iter 2615/5233 - loss 0.31585696\n",
            "2019-06-18 18:18:31,182 epoch 14 - iter 3138/5233 - loss 0.31857410\n",
            "2019-06-18 18:19:07,751 epoch 14 - iter 3661/5233 - loss 0.31880630\n",
            "2019-06-18 18:19:44,065 epoch 14 - iter 4184/5233 - loss 0.31801713\n",
            "2019-06-18 18:20:20,418 epoch 14 - iter 4707/5233 - loss 0.31909730\n",
            "2019-06-18 18:20:56,845 epoch 14 - iter 5230/5233 - loss 0.31944262\n",
            "2019-06-18 18:20:57,248 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:20:57,250 EPOCH 14 done: loss 0.3194 - lr 0.1000 - bad epochs 0\n",
            "2019-06-18 18:21:24,887 DEV : loss 0.3016117811203003 - score 0.1312\n",
            "2019-06-18 18:22:37,073 TEST : loss 0.28558245301246643 - score 0.1429\n",
            "2019-06-18 18:22:37,075 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:22:37,724 epoch 15 - iter 0/5233 - loss 0.43495598\n",
            "2019-06-18 18:23:14,432 epoch 15 - iter 523/5233 - loss 0.32425961\n",
            "2019-06-18 18:23:51,010 epoch 15 - iter 1046/5233 - loss 0.32513309\n",
            "2019-06-18 18:24:27,594 epoch 15 - iter 1569/5233 - loss 0.32516141\n",
            "2019-06-18 18:25:04,027 epoch 15 - iter 2092/5233 - loss 0.32056441\n",
            "2019-06-18 18:25:40,716 epoch 15 - iter 2615/5233 - loss 0.31864913\n",
            "2019-06-18 18:26:17,040 epoch 15 - iter 3138/5233 - loss 0.31961843\n",
            "2019-06-18 18:26:53,434 epoch 15 - iter 3661/5233 - loss 0.31901925\n",
            "2019-06-18 18:27:29,921 epoch 15 - iter 4184/5233 - loss 0.31978387\n",
            "2019-06-18 18:28:06,362 epoch 15 - iter 4707/5233 - loss 0.31930126\n",
            "2019-06-18 18:28:42,663 epoch 15 - iter 5230/5233 - loss 0.31912792\n",
            "2019-06-18 18:28:43,074 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:28:43,082 EPOCH 15 done: loss 0.3191 - lr 0.1000 - bad epochs 1\n",
            "2019-06-18 18:29:10,590 DEV : loss 0.3007791340351105 - score 0.117\n",
            "2019-06-18 18:30:21,436 TEST : loss 0.2846558094024658 - score 0.135\n",
            "2019-06-18 18:30:21,438 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:30:22,101 epoch 16 - iter 0/5233 - loss 0.39828438\n",
            "2019-06-18 18:30:58,704 epoch 16 - iter 523/5233 - loss 0.31829719\n",
            "2019-06-18 18:31:35,363 epoch 16 - iter 1046/5233 - loss 0.31756056\n",
            "2019-06-18 18:32:11,786 epoch 16 - iter 1569/5233 - loss 0.31853606\n",
            "2019-06-18 18:32:48,037 epoch 16 - iter 2092/5233 - loss 0.31848990\n",
            "2019-06-18 18:33:24,409 epoch 16 - iter 2615/5233 - loss 0.31688584\n",
            "2019-06-18 18:34:00,615 epoch 16 - iter 3138/5233 - loss 0.31773287\n",
            "2019-06-18 18:34:36,796 epoch 16 - iter 3661/5233 - loss 0.31808133\n",
            "2019-06-18 18:35:13,182 epoch 16 - iter 4184/5233 - loss 0.31771089\n",
            "2019-06-18 18:35:49,696 epoch 16 - iter 4707/5233 - loss 0.31749571\n",
            "2019-06-18 18:36:25,897 epoch 16 - iter 5230/5233 - loss 0.31813737\n",
            "2019-06-18 18:36:26,310 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:36:26,313 EPOCH 16 done: loss 0.3181 - lr 0.1000 - bad epochs 2\n",
            "2019-06-18 18:36:53,836 DEV : loss 0.3016583323478699 - score 0.1179\n",
            "2019-06-18 18:38:04,799 TEST : loss 0.2841161787509918 - score 0.136\n",
            "2019-06-18 18:38:04,802 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:38:05,475 epoch 17 - iter 0/5233 - loss 0.19602239\n",
            "2019-06-18 18:38:41,977 epoch 17 - iter 523/5233 - loss 0.31171934\n",
            "2019-06-18 18:39:18,432 epoch 17 - iter 1046/5233 - loss 0.30983213\n",
            "2019-06-18 18:39:55,131 epoch 17 - iter 1569/5233 - loss 0.31682359\n",
            "2019-06-18 18:40:31,440 epoch 17 - iter 2092/5233 - loss 0.31821053\n",
            "2019-06-18 18:41:07,906 epoch 17 - iter 2615/5233 - loss 0.32183723\n",
            "2019-06-18 18:41:44,516 epoch 17 - iter 3138/5233 - loss 0.32010254\n",
            "2019-06-18 18:42:20,932 epoch 17 - iter 3661/5233 - loss 0.31889933\n",
            "2019-06-18 18:42:57,293 epoch 17 - iter 4184/5233 - loss 0.31798100\n",
            "2019-06-18 18:43:33,746 epoch 17 - iter 4707/5233 - loss 0.31868817\n",
            "2019-06-18 18:44:10,214 epoch 17 - iter 5230/5233 - loss 0.31792659\n",
            "2019-06-18 18:44:10,629 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:44:10,632 EPOCH 17 done: loss 0.3180 - lr 0.1000 - bad epochs 3\n",
            "2019-06-18 18:44:38,096 DEV : loss 0.3004593849182129 - score 0.1379\n",
            "2019-06-18 18:45:51,283 TEST : loss 0.2840838134288788 - score 0.1492\n",
            "Epoch    16: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2019-06-18 18:45:51,285 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:45:51,951 epoch 18 - iter 0/5233 - loss 0.32196271\n",
            "2019-06-18 18:46:28,709 epoch 18 - iter 523/5233 - loss 0.31400643\n",
            "2019-06-18 18:47:05,257 epoch 18 - iter 1046/5233 - loss 0.31432488\n",
            "2019-06-18 18:47:42,028 epoch 18 - iter 1569/5233 - loss 0.31362942\n",
            "2019-06-18 18:48:18,578 epoch 18 - iter 2092/5233 - loss 0.31295564\n",
            "2019-06-18 18:48:54,950 epoch 18 - iter 2615/5233 - loss 0.31298387\n",
            "2019-06-18 18:49:31,489 epoch 18 - iter 3138/5233 - loss 0.31418944\n",
            "2019-06-18 18:50:08,292 epoch 18 - iter 3661/5233 - loss 0.31591609\n",
            "2019-06-18 18:50:44,910 epoch 18 - iter 4184/5233 - loss 0.31689925\n",
            "2019-06-18 18:51:21,441 epoch 18 - iter 4707/5233 - loss 0.31572177\n",
            "2019-06-18 18:51:57,906 epoch 18 - iter 5230/5233 - loss 0.31558240\n",
            "2019-06-18 18:51:58,302 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:51:58,304 EPOCH 18 done: loss 0.3156 - lr 0.0500 - bad epochs 0\n",
            "2019-06-18 18:52:25,904 DEV : loss 0.29831358790397644 - score 0.1269\n",
            "2019-06-18 18:53:37,017 TEST : loss 0.28195521235466003 - score 0.1462\n",
            "2019-06-18 18:53:37,019 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:53:37,685 epoch 19 - iter 0/5233 - loss 0.12226184\n",
            "2019-06-18 18:54:14,328 epoch 19 - iter 523/5233 - loss 0.30647068\n",
            "2019-06-18 18:54:50,801 epoch 19 - iter 1046/5233 - loss 0.31411932\n",
            "2019-06-18 18:55:27,527 epoch 19 - iter 1569/5233 - loss 0.31448540\n",
            "2019-06-18 18:56:03,924 epoch 19 - iter 2092/5233 - loss 0.31087376\n",
            "2019-06-18 18:56:40,427 epoch 19 - iter 2615/5233 - loss 0.31195418\n",
            "2019-06-18 18:57:16,868 epoch 19 - iter 3138/5233 - loss 0.31399552\n",
            "2019-06-18 18:57:53,235 epoch 19 - iter 3661/5233 - loss 0.31451084\n",
            "2019-06-18 18:58:29,537 epoch 19 - iter 4184/5233 - loss 0.31521440\n",
            "2019-06-18 18:59:05,742 epoch 19 - iter 4707/5233 - loss 0.31601341\n",
            "2019-06-18 18:59:41,983 epoch 19 - iter 5230/5233 - loss 0.31632324\n",
            "2019-06-18 18:59:42,412 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 18:59:42,413 EPOCH 19 done: loss 0.3164 - lr 0.0500 - bad epochs 1\n",
            "2019-06-18 19:00:09,554 DEV : loss 0.2988254427909851 - score 0.1464\n",
            "2019-06-18 19:01:20,654 TEST : loss 0.2823787033557892 - score 0.1547\n",
            "2019-06-18 19:01:24,717 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:01:25,360 epoch 20 - iter 0/5233 - loss 0.17801267\n",
            "2019-06-18 19:02:01,746 epoch 20 - iter 523/5233 - loss 0.31779345\n",
            "2019-06-18 19:02:37,863 epoch 20 - iter 1046/5233 - loss 0.32236548\n",
            "2019-06-18 19:03:13,988 epoch 20 - iter 1569/5233 - loss 0.32320609\n",
            "2019-06-18 19:03:50,028 epoch 20 - iter 2092/5233 - loss 0.32020139\n",
            "2019-06-18 19:04:26,086 epoch 20 - iter 2615/5233 - loss 0.31632020\n",
            "2019-06-18 19:05:02,131 epoch 20 - iter 3138/5233 - loss 0.31588918\n",
            "2019-06-18 19:05:38,368 epoch 20 - iter 3661/5233 - loss 0.31434450\n",
            "2019-06-18 19:06:14,621 epoch 20 - iter 4184/5233 - loss 0.31438193\n",
            "2019-06-18 19:06:50,785 epoch 20 - iter 4707/5233 - loss 0.31409841\n",
            "2019-06-18 19:07:27,020 epoch 20 - iter 5230/5233 - loss 0.31482849\n",
            "2019-06-18 19:07:27,422 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:07:27,424 EPOCH 20 done: loss 0.3148 - lr 0.0500 - bad epochs 0\n",
            "2019-06-18 19:07:54,962 DEV : loss 0.2978552579879761 - score 0.1484\n",
            "2019-06-18 19:09:05,964 TEST : loss 0.2817593514919281 - score 0.1546\n",
            "2019-06-18 19:09:10,043 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:09:10,691 epoch 21 - iter 0/5233 - loss 0.37978190\n",
            "2019-06-18 19:09:49,288 epoch 21 - iter 523/5233 - loss 0.31682590\n",
            "2019-06-18 19:10:25,599 epoch 21 - iter 1046/5233 - loss 0.31826337\n",
            "2019-06-18 19:11:02,030 epoch 21 - iter 1569/5233 - loss 0.31504413\n",
            "2019-06-18 19:11:38,495 epoch 21 - iter 2092/5233 - loss 0.31411650\n",
            "2019-06-18 19:12:14,717 epoch 21 - iter 2615/5233 - loss 0.31642977\n",
            "2019-06-18 19:12:50,909 epoch 21 - iter 3138/5233 - loss 0.31502443\n",
            "2019-06-18 19:13:27,284 epoch 21 - iter 3661/5233 - loss 0.31631687\n",
            "2019-06-18 19:14:03,066 epoch 21 - iter 4184/5233 - loss 0.31534273\n",
            "2019-06-18 19:14:38,816 epoch 21 - iter 4707/5233 - loss 0.31469169\n",
            "2019-06-18 19:15:14,925 epoch 21 - iter 5230/5233 - loss 0.31546434\n",
            "2019-06-18 19:15:15,336 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:15:15,338 EPOCH 21 done: loss 0.3155 - lr 0.0500 - bad epochs 0\n",
            "2019-06-18 19:15:42,143 DEV : loss 0.29775598645210266 - score 0.119\n",
            "2019-06-18 19:16:52,677 TEST : loss 0.28071892261505127 - score 0.1365\n",
            "2019-06-18 19:16:52,679 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:16:53,343 epoch 22 - iter 0/5233 - loss 0.29170540\n",
            "2019-06-18 19:17:29,286 epoch 22 - iter 523/5233 - loss 0.31523361\n",
            "2019-06-18 19:18:05,180 epoch 22 - iter 1046/5233 - loss 0.30883620\n",
            "2019-06-18 19:18:40,906 epoch 22 - iter 1569/5233 - loss 0.30911977\n",
            "2019-06-18 19:19:16,742 epoch 22 - iter 2092/5233 - loss 0.31008673\n",
            "2019-06-18 19:19:52,864 epoch 22 - iter 2615/5233 - loss 0.31320307\n",
            "2019-06-18 19:20:28,766 epoch 22 - iter 3138/5233 - loss 0.31366220\n",
            "2019-06-18 19:21:04,856 epoch 22 - iter 3661/5233 - loss 0.31441245\n",
            "2019-06-18 19:21:41,307 epoch 22 - iter 4184/5233 - loss 0.31385514\n",
            "2019-06-18 19:22:17,565 epoch 22 - iter 4707/5233 - loss 0.31330340\n",
            "2019-06-18 19:22:53,746 epoch 22 - iter 5230/5233 - loss 0.31429735\n",
            "2019-06-18 19:22:54,188 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:22:54,191 EPOCH 22 done: loss 0.3144 - lr 0.0500 - bad epochs 1\n",
            "2019-06-18 19:23:21,781 DEV : loss 0.29755842685699463 - score 0.1346\n",
            "2019-06-18 19:24:32,707 TEST : loss 0.2810210883617401 - score 0.1487\n",
            "2019-06-18 19:24:32,710 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:24:33,369 epoch 23 - iter 0/5233 - loss 0.14246899\n",
            "2019-06-18 19:25:09,762 epoch 23 - iter 523/5233 - loss 0.32978847\n",
            "2019-06-18 19:25:46,051 epoch 23 - iter 1046/5233 - loss 0.32138773\n",
            "2019-06-18 19:26:22,183 epoch 23 - iter 1569/5233 - loss 0.31816226\n",
            "2019-06-18 19:26:58,415 epoch 23 - iter 2092/5233 - loss 0.31693281\n",
            "2019-06-18 19:27:34,765 epoch 23 - iter 2615/5233 - loss 0.31763736\n",
            "2019-06-18 19:28:11,067 epoch 23 - iter 3138/5233 - loss 0.31457065\n",
            "2019-06-18 19:28:47,152 epoch 23 - iter 3661/5233 - loss 0.31488876\n",
            "2019-06-18 19:29:23,250 epoch 23 - iter 4184/5233 - loss 0.31540886\n",
            "2019-06-18 19:29:59,263 epoch 23 - iter 4707/5233 - loss 0.31454123\n",
            "2019-06-18 19:30:35,309 epoch 23 - iter 5230/5233 - loss 0.31498137\n",
            "2019-06-18 19:30:35,733 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:30:35,734 EPOCH 23 done: loss 0.3150 - lr 0.0500 - bad epochs 2\n",
            "2019-06-18 19:31:03,267 DEV : loss 0.2962861657142639 - score 0.1312\n",
            "2019-06-18 19:32:14,053 TEST : loss 0.28115639090538025 - score 0.1542\n",
            "2019-06-18 19:32:14,055 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:32:14,708 epoch 24 - iter 0/5233 - loss 0.21977580\n",
            "2019-06-18 19:32:50,897 epoch 24 - iter 523/5233 - loss 0.31113474\n",
            "2019-06-18 19:33:29,144 epoch 24 - iter 1046/5233 - loss 0.32002218\n",
            "2019-06-18 19:34:05,422 epoch 24 - iter 1569/5233 - loss 0.31950756\n",
            "2019-06-18 19:34:41,880 epoch 24 - iter 2092/5233 - loss 0.31643593\n",
            "2019-06-18 19:35:18,174 epoch 24 - iter 2615/5233 - loss 0.31461126\n",
            "2019-06-18 19:35:54,408 epoch 24 - iter 3138/5233 - loss 0.31373895\n",
            "2019-06-18 19:36:30,664 epoch 24 - iter 3661/5233 - loss 0.31305183\n",
            "2019-06-18 19:37:07,196 epoch 24 - iter 4184/5233 - loss 0.31303235\n",
            "2019-06-18 19:37:43,463 epoch 24 - iter 4707/5233 - loss 0.31321242\n",
            "2019-06-18 19:38:19,639 epoch 24 - iter 5230/5233 - loss 0.31405852\n",
            "2019-06-18 19:38:20,054 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:38:20,056 EPOCH 24 done: loss 0.3141 - lr 0.0500 - bad epochs 3\n",
            "2019-06-18 19:38:47,469 DEV : loss 0.2971523106098175 - score 0.1277\n",
            "2019-06-18 19:39:58,384 TEST : loss 0.2818151116371155 - score 0.1499\n",
            "Epoch    23: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2019-06-18 19:39:58,387 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:39:59,049 epoch 25 - iter 0/5233 - loss 0.14293152\n",
            "2019-06-18 19:40:35,721 epoch 25 - iter 523/5233 - loss 0.31049664\n",
            "2019-06-18 19:41:12,175 epoch 25 - iter 1046/5233 - loss 0.31501544\n",
            "2019-06-18 19:41:48,510 epoch 25 - iter 1569/5233 - loss 0.31160426\n",
            "2019-06-18 19:42:25,027 epoch 25 - iter 2092/5233 - loss 0.31543621\n",
            "2019-06-18 19:43:01,335 epoch 25 - iter 2615/5233 - loss 0.31245203\n",
            "2019-06-18 19:43:37,686 epoch 25 - iter 3138/5233 - loss 0.31151689\n",
            "2019-06-18 19:44:13,839 epoch 25 - iter 3661/5233 - loss 0.31206462\n",
            "2019-06-18 19:44:50,020 epoch 25 - iter 4184/5233 - loss 0.31246670\n",
            "2019-06-18 19:45:26,229 epoch 25 - iter 4707/5233 - loss 0.31148496\n",
            "2019-06-18 19:46:02,380 epoch 25 - iter 5230/5233 - loss 0.31229915\n",
            "2019-06-18 19:46:02,789 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:46:02,791 EPOCH 25 done: loss 0.3124 - lr 0.0250 - bad epochs 0\n",
            "2019-06-18 19:46:29,575 DEV : loss 0.2953387200832367 - score 0.1567\n",
            "2019-06-18 19:47:38,552 TEST : loss 0.28093329071998596 - score 0.1693\n",
            "2019-06-18 19:47:42,569 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:47:43,244 epoch 26 - iter 0/5233 - loss 0.13615441\n",
            "2019-06-18 19:48:19,683 epoch 26 - iter 523/5233 - loss 0.31244123\n",
            "2019-06-18 19:48:56,047 epoch 26 - iter 1046/5233 - loss 0.31518475\n",
            "2019-06-18 19:49:32,390 epoch 26 - iter 1569/5233 - loss 0.31568839\n",
            "2019-06-18 19:50:08,587 epoch 26 - iter 2092/5233 - loss 0.31735963\n",
            "2019-06-18 19:50:44,793 epoch 26 - iter 2615/5233 - loss 0.31459065\n",
            "2019-06-18 19:51:21,033 epoch 26 - iter 3138/5233 - loss 0.31302847\n",
            "2019-06-18 19:51:57,231 epoch 26 - iter 3661/5233 - loss 0.31271473\n",
            "2019-06-18 19:52:33,495 epoch 26 - iter 4184/5233 - loss 0.31288416\n",
            "2019-06-18 19:53:09,471 epoch 26 - iter 4707/5233 - loss 0.31235500\n",
            "2019-06-18 19:53:45,637 epoch 26 - iter 5230/5233 - loss 0.31205223\n",
            "2019-06-18 19:53:46,061 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:53:46,064 EPOCH 26 done: loss 0.3120 - lr 0.0250 - bad epochs 0\n",
            "2019-06-18 19:54:13,585 DEV : loss 0.29540184140205383 - score 0.1468\n",
            "2019-06-18 19:55:24,761 TEST : loss 0.28092533349990845 - score 0.1594\n",
            "2019-06-18 19:55:24,763 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 19:55:25,432 epoch 27 - iter 0/5233 - loss 0.45239082\n",
            "2019-06-18 19:56:01,890 epoch 27 - iter 523/5233 - loss 0.31298751\n",
            "2019-06-18 19:56:38,105 epoch 27 - iter 1046/5233 - loss 0.31587296\n",
            "2019-06-18 19:57:14,357 epoch 27 - iter 1569/5233 - loss 0.31619890\n",
            "2019-06-18 19:57:53,002 epoch 27 - iter 2092/5233 - loss 0.31330278\n",
            "2019-06-18 19:58:29,124 epoch 27 - iter 2615/5233 - loss 0.31180897\n",
            "2019-06-18 19:59:05,274 epoch 27 - iter 3138/5233 - loss 0.31197769\n",
            "2019-06-18 19:59:41,537 epoch 27 - iter 3661/5233 - loss 0.31304314\n",
            "2019-06-18 20:00:17,658 epoch 27 - iter 4184/5233 - loss 0.31352646\n",
            "2019-06-18 20:00:53,773 epoch 27 - iter 4707/5233 - loss 0.31297974\n",
            "2019-06-18 20:01:30,001 epoch 27 - iter 5230/5233 - loss 0.31263043\n",
            "2019-06-18 20:01:30,420 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:01:30,422 EPOCH 27 done: loss 0.3126 - lr 0.0250 - bad epochs 1\n",
            "2019-06-18 20:01:57,842 DEV : loss 0.2957346737384796 - score 0.1426\n",
            "2019-06-18 20:03:08,818 TEST : loss 0.28097423911094666 - score 0.1562\n",
            "2019-06-18 20:03:08,820 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:03:09,492 epoch 28 - iter 0/5233 - loss 0.32328731\n",
            "2019-06-18 20:03:45,892 epoch 28 - iter 523/5233 - loss 0.31186836\n",
            "2019-06-18 20:04:22,100 epoch 28 - iter 1046/5233 - loss 0.31060028\n",
            "2019-06-18 20:04:58,389 epoch 28 - iter 1569/5233 - loss 0.31071348\n",
            "2019-06-18 20:05:34,726 epoch 28 - iter 2092/5233 - loss 0.31176534\n",
            "2019-06-18 20:06:10,843 epoch 28 - iter 2615/5233 - loss 0.31199150\n",
            "2019-06-18 20:06:46,877 epoch 28 - iter 3138/5233 - loss 0.31096491\n",
            "2019-06-18 20:07:23,038 epoch 28 - iter 3661/5233 - loss 0.31379747\n",
            "2019-06-18 20:07:59,401 epoch 28 - iter 4184/5233 - loss 0.31423673\n",
            "2019-06-18 20:08:35,484 epoch 28 - iter 4707/5233 - loss 0.31312810\n",
            "2019-06-18 20:09:11,468 epoch 28 - iter 5230/5233 - loss 0.31299674\n",
            "2019-06-18 20:09:11,877 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:09:11,879 EPOCH 28 done: loss 0.3130 - lr 0.0250 - bad epochs 2\n",
            "2019-06-18 20:09:39,252 DEV : loss 0.2954722046852112 - score 0.1554\n",
            "2019-06-18 20:10:49,582 TEST : loss 0.28009581565856934 - score 0.1647\n",
            "2019-06-18 20:10:49,584 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:10:50,247 epoch 29 - iter 0/5233 - loss 0.32965896\n",
            "2019-06-18 20:11:26,743 epoch 29 - iter 523/5233 - loss 0.31665458\n",
            "2019-06-18 20:12:03,259 epoch 29 - iter 1046/5233 - loss 0.30865444\n",
            "2019-06-18 20:12:39,651 epoch 29 - iter 1569/5233 - loss 0.31102193\n",
            "2019-06-18 20:13:16,250 epoch 29 - iter 2092/5233 - loss 0.31302843\n",
            "2019-06-18 20:13:52,931 epoch 29 - iter 2615/5233 - loss 0.31458896\n",
            "2019-06-18 20:14:29,251 epoch 29 - iter 3138/5233 - loss 0.31289611\n",
            "2019-06-18 20:15:05,667 epoch 29 - iter 3661/5233 - loss 0.31252145\n",
            "2019-06-18 20:15:42,224 epoch 29 - iter 4184/5233 - loss 0.31251526\n",
            "2019-06-18 20:16:18,600 epoch 29 - iter 4707/5233 - loss 0.31297217\n",
            "2019-06-18 20:16:54,962 epoch 29 - iter 5230/5233 - loss 0.31168409\n",
            "2019-06-18 20:16:55,392 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:16:55,394 EPOCH 29 done: loss 0.3117 - lr 0.0250 - bad epochs 3\n",
            "2019-06-18 20:17:23,205 DEV : loss 0.295558899641037 - score 0.1497\n",
            "2019-06-18 20:18:34,597 TEST : loss 0.27953219413757324 - score 0.1626\n",
            "Epoch    28: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2019-06-18 20:18:34,600 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:18:35,273 epoch 30 - iter 0/5233 - loss 0.46602398\n",
            "2019-06-18 20:19:11,916 epoch 30 - iter 523/5233 - loss 0.30918942\n",
            "2019-06-18 20:19:48,362 epoch 30 - iter 1046/5233 - loss 0.30581537\n",
            "2019-06-18 20:20:24,915 epoch 30 - iter 1569/5233 - loss 0.31172503\n",
            "2019-06-18 20:21:01,352 epoch 30 - iter 2092/5233 - loss 0.31034745\n",
            "2019-06-18 20:21:37,418 epoch 30 - iter 2615/5233 - loss 0.31086654\n",
            "2019-06-18 20:22:13,885 epoch 30 - iter 3138/5233 - loss 0.31077321\n",
            "2019-06-18 20:22:52,524 epoch 30 - iter 3661/5233 - loss 0.31222750\n",
            "2019-06-18 20:23:29,058 epoch 30 - iter 4184/5233 - loss 0.31300607\n",
            "2019-06-18 20:24:05,712 epoch 30 - iter 4707/5233 - loss 0.31180617\n",
            "2019-06-18 20:24:42,049 epoch 30 - iter 5230/5233 - loss 0.31151211\n",
            "2019-06-18 20:24:42,482 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:24:42,484 EPOCH 30 done: loss 0.3115 - lr 0.0125 - bad epochs 0\n",
            "2019-06-18 20:25:10,255 DEV : loss 0.29510846734046936 - score 0.1375\n",
            "2019-06-18 20:26:21,727 TEST : loss 0.2797931730747223 - score 0.1532\n",
            "2019-06-18 20:26:21,729 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:26:22,393 epoch 31 - iter 0/5233 - loss 0.28467089\n",
            "2019-06-18 20:26:59,098 epoch 31 - iter 523/5233 - loss 0.32087264\n",
            "2019-06-18 20:27:35,779 epoch 31 - iter 1046/5233 - loss 0.31635603\n",
            "2019-06-18 20:27:43,706 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:27:43,709 Exiting from training early.\n",
            "2019-06-18 20:27:43,716 Saving model ...\n",
            "2019-06-18 20:27:47,793 Done.\n",
            "2019-06-18 20:27:47,795 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-18 20:27:47,797 Testing using best model ...\n",
            "2019-06-18 20:27:47,799 loading file resources/taggers/resume-ner/best-model.pt\n",
            "2019-06-18 20:29:00,355 0.4726\t0.1031\t0.1693\n",
            "2019-06-18 20:29:00,357 \n",
            "MICRO_AVG: acc 0.0925 - f1-score 0.1693\n",
            "MACRO_AVG: acc 0.1115 - f1-score 0.186975\n",
            "-          tp: 0 - fp: 0 - fn: 857 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "Companies  tp: 256 - fp: 317 - fn: 727 - tn: 256 - precision: 0.4468 - recall: 0.2604 - accuracy: 0.1969 - f1-score: 0.3290\n",
            "Degree     tp: 86 - fp: 52 - fn: 257 - tn: 86 - precision: 0.6232 - recall: 0.2507 - accuracy: 0.2177 - f1-score: 0.3576\n",
            "Skills     tp: 55 - fp: 74 - fn: 1611 - tn: 55 - precision: 0.4264 - recall: 0.0330 - accuracy: 0.0316 - f1-score: 0.0613\n",
            "2019-06-18 20:29:00,363 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [tensor(0.3289, device='cuda:0'),\n",
              "  tensor(0.3259, device='cuda:0'),\n",
              "  tensor(0.3213, device='cuda:0'),\n",
              "  tensor(0.3191, device='cuda:0'),\n",
              "  tensor(0.3153, device='cuda:0'),\n",
              "  tensor(0.3122, device='cuda:0'),\n",
              "  tensor(0.3113, device='cuda:0'),\n",
              "  tensor(0.3080, device='cuda:0'),\n",
              "  tensor(0.3106, device='cuda:0'),\n",
              "  tensor(0.3121, device='cuda:0'),\n",
              "  tensor(0.3065, device='cuda:0'),\n",
              "  tensor(0.3078, device='cuda:0'),\n",
              "  tensor(0.3042, device='cuda:0'),\n",
              "  tensor(0.3016, device='cuda:0'),\n",
              "  tensor(0.3008, device='cuda:0'),\n",
              "  tensor(0.3017, device='cuda:0'),\n",
              "  tensor(0.3005, device='cuda:0'),\n",
              "  tensor(0.2983, device='cuda:0'),\n",
              "  tensor(0.2988, device='cuda:0'),\n",
              "  tensor(0.2979, device='cuda:0'),\n",
              "  tensor(0.2978, device='cuda:0'),\n",
              "  tensor(0.2976, device='cuda:0'),\n",
              "  tensor(0.2963, device='cuda:0'),\n",
              "  tensor(0.2972, device='cuda:0'),\n",
              "  tensor(0.2953, device='cuda:0'),\n",
              "  tensor(0.2954, device='cuda:0'),\n",
              "  tensor(0.2957, device='cuda:0'),\n",
              "  tensor(0.2955, device='cuda:0'),\n",
              "  tensor(0.2956, device='cuda:0'),\n",
              "  tensor(0.2951, device='cuda:0')],\n",
              " 'dev_score_history': [0.0202,\n",
              "  0.0533,\n",
              "  0.0887,\n",
              "  0.0653,\n",
              "  0.0952,\n",
              "  0.1029,\n",
              "  0.0995,\n",
              "  0.1014,\n",
              "  0.1107,\n",
              "  0.0942,\n",
              "  0.1161,\n",
              "  0.1389,\n",
              "  0.1428,\n",
              "  0.1312,\n",
              "  0.117,\n",
              "  0.1179,\n",
              "  0.1379,\n",
              "  0.1269,\n",
              "  0.1464,\n",
              "  0.1484,\n",
              "  0.119,\n",
              "  0.1346,\n",
              "  0.1312,\n",
              "  0.1277,\n",
              "  0.1567,\n",
              "  0.1468,\n",
              "  0.1426,\n",
              "  0.1554,\n",
              "  0.1497,\n",
              "  0.1375],\n",
              " 'test_score': 0.1693,\n",
              " 'train_loss_history': [0.3558838156473903,\n",
              "  0.33916260759381245,\n",
              "  0.336845900051861,\n",
              "  0.33405060068088943,\n",
              "  0.33105784922072223,\n",
              "  0.32802037411641144,\n",
              "  0.326688277409315,\n",
              "  0.3262289474618075,\n",
              "  0.3244529908721223,\n",
              "  0.3236640360161517,\n",
              "  0.3218048342823777,\n",
              "  0.3211407386002852,\n",
              "  0.321918916125929,\n",
              "  0.31942883611704953,\n",
              "  0.31908177756970457,\n",
              "  0.31805605745733573,\n",
              "  0.31797844111583395,\n",
              "  0.3155558329269919,\n",
              "  0.31642408619163004,\n",
              "  0.31478408682068426,\n",
              "  0.31546662658447294,\n",
              "  0.31436049741437094,\n",
              "  0.3149751181652481,\n",
              "  0.3140662563902435,\n",
              "  0.31241728872567637,\n",
              "  0.31202813066982094,\n",
              "  0.3126427548170978,\n",
              "  0.3129713059300313,\n",
              "  0.3117190366527238,\n",
              "  0.3115362645707599]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg7AHDGcbcbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 8. plot training curves (optional)\n",
        "from flair.visual.training_curves import Plotter\n",
        "plotter = Plotter()\n",
        "plotter.plot_training_curves('resources/taggers/resume-ner/loss.tsv')\n",
        "plotter.plot_weights('resources/taggers/resume-ner/weights.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}