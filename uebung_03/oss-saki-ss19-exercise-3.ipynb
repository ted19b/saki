{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "# Exercise 3 - SAKI\n",
    "\n",
    "The purpose of this exercise is to develop an algorithm that optimizes the route that a robot takes for pick-up and storage of items in a warehouse.\n",
    "\n",
    "There are the following constraints : \n",
    "\n",
    "* Size of warehouse is {1..3} x {1..3}\n",
    "* There is separate start/stop position outside the 3x3 storage space\n",
    "* The first position the robot can move into is always (1, 1)\n",
    "* Robots can move to adjacent fields (but not diagonally)\n",
    "* There are three types of items, identified by color (white, blue, red)\n",
    "\n",
    "\n",
    "Here are the main lines of our work plan\n",
    "* I. Introduction \n",
    "\n",
    "* II. Data Analysis / Data Extraction\n",
    "\n",
    "* III. Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "Reinforcement learning (RL) is learning what to do, how to map situations to actions, so as to maximize\n",
    "a numerical reward signal. The learner is not told which actions to take, but instead must discover\n",
    "which actions yield the most reward by trying them.\n",
    "\n",
    "In short, we can say that reinforcement learning is learning by trying.\n",
    "\n",
    "In RL there is no supervisor, only a reward signal or a real number that tells the agent how good or bad was his action. \n",
    "Feedback from the environment might be delayed over several time steps, it’s not necessarily instantaneous \n",
    "e.g. for the task of reaching a goal in a grid-world, the feedback might be at the end when the agent reaches the goal. \n",
    "The agent might spend some time exploring and wandering in the environment until it finally reaches the goal after a while to realize what were the good and bad actions it has taken.\n",
    "\n",
    "In machine learning or supervised learning, we have a dataset that describes the environment to the algorithm and the right answers or actions to do when faced with a specific situation, and the algorithm tries to generalize from that data to new situations.\n",
    "\n",
    "In RL the agent influences the environment through its actions which in turn affect the subsequent data it receives from the environment, it’s an active learning process.\n",
    "\n",
    "In the problem, an agent is supposed to decide the best action to select based on his current state. When this step is repeated, the problem is known as a __Markov Decision Process__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mdptoolbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cf23df04292f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmdptoolbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mdptoolbox'"
     ]
    }
   ],
   "source": [
    "# We start by importing all the librairies necessary for ourn work.\n",
    "import csv\n",
    "from itertools import product\n",
    "import mdptoolbox\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Analysis / Data Extraction\n",
    "\n",
    "Before defining the different parameters of the Markov Decision Process, we will start by importing our different data for analysis. \n",
    "\n",
    "We have 2 files at our disposal representing the training set and the testing set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "    action  color\n",
      "0    store    red\n",
      "1    store    red\n",
      "2    store    red\n",
      "3    store  white\n",
      "4  restore    red\n",
      "----- -----\n",
      "Testing Set\n",
      "    action  color\n",
      "0    store    red\n",
      "1    store  white\n",
      "2  restore  white\n",
      "3    store    red\n",
      "4    store   blue\n"
     ]
    }
   ],
   "source": [
    "# import file training and testing file into a variable\n",
    "\n",
    "training_data_set = pd.read_csv(\"data/SAKI Exercise 3 warehousetraining2x2.txt\", delimiter='\\t', names=[\"action\", \"color\"])\n",
    "testing_data_set = pd.read_csv(\"data/SAKI Exercise 3 warehouseorder2x2.txt\", delimiter='\\t', names=[\"action\", \"color\"])\n",
    "\n",
    "print('Training Set')\n",
    "print(training_data_set.head())\n",
    "print('----- -----')\n",
    "print('Testing Set')\n",
    "print(testing_data_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported our training and test data. \n",
    "\n",
    "We will calculate the distribution of the different items and actions in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>color</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>restore</td>\n",
       "      <td>blue</td>\n",
       "      <td>0.121683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>restore</td>\n",
       "      <td>red</td>\n",
       "      <td>0.252415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restore</td>\n",
       "      <td>white</td>\n",
       "      <td>0.125841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>store</td>\n",
       "      <td>blue</td>\n",
       "      <td>0.121683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store</td>\n",
       "      <td>red</td>\n",
       "      <td>0.252415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>store</td>\n",
       "      <td>white</td>\n",
       "      <td>0.125963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    action  color     count\n",
       "0  restore   blue  0.121683\n",
       "1  restore    red  0.252415\n",
       "2  restore  white  0.125841\n",
       "3    store   blue  0.121683\n",
       "4    store    red  0.252415\n",
       "5    store  white  0.125963"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_training = training_data_set.copy()\n",
    "distribution_training = distribution_training.groupby(['action', 'color']).size().reset_index(name='count')\n",
    "distribution_training['count'] = distribution_training['count'].div(len(training_data_set))\n",
    "\n",
    "distribution_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Markov Decision Process\n",
    "\n",
    "The Markov decision process, better known as MDP, is an approach in reinforcement learning to take decisions in a gridworld environment. A gridworld environment consists of states in the form of grids.\n",
    "\n",
    "Markov decision processes formally describe an environment for reinforcement learning, where the environment is fully observable ie the current state completely characterises the process\n",
    "\n",
    "Markov property : \"The future is independent of the past given the present\"\n",
    "\n",
    "Thus, any reinforcement learning task composed of a set of states, actions, and rewards that follows the Markov property would be considered an MDP.\n",
    "\n",
    "The solution to an MDP is called a policy and the objective is to find the optimal policy for that MDP task.\n",
    "\n",
    "A Markov Decision Process (MDP) model contains:\n",
    "\n",
    "* A set of possible world states S.\n",
    "* A set of Models.\n",
    "* A set of possible actions A.\n",
    "* A real valued reward function R(s,a).\n",
    "* A policy the solution of Markov Decision Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Actions\n",
    "\n",
    "A is a set of all possible action. A(s) defines the set of actions that can be taken being in state S.\n",
    "\n",
    "Our robot can either retrieve or deposit objects in the warehouse. However, it should be noted that we have 3 different types of items, which will make us take different actions depending on the item chosen.\n",
    "\n",
    "in short, our robot will be able to perform the different actions: \n",
    "\n",
    "* pick up or store red item --> pick_up_red, store_red\n",
    "* pick up or store blue item --> pick_up_blue, store_blue\n",
    "* pick up or store white item --> pick_up_white, store_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = ['pick_up_red', 'pick_up_blue', 'pick_up_white', 'store_red', 'store_blue', 'store_white']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## 2. States\n",
    "\n",
    "A State is a set of tokens that represent every state that the agent can be in.\n",
    "\n",
    "Given our environment (warehouse 2*2) we can have different states depending on the items that would be present in the warehouse. In short, to obtain our states we must think of all the possible combinations in which our environment could be found. \n",
    "\n",
    "For example: at the beginning our warehouse is empty, or the warehouse already contains a red item or the warehouse already contains 2 blue items...\n",
    "\n",
    "we can therefore see that each of the above scenarios constitutes different states of our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('empty', 'empty', 'empty', 'empty', 'pick_up_red'), ('empty', 'empty', 'empty', 'empty', 'pick_up_blue'), ('empty', 'empty', 'empty', 'empty', 'pick_up_white'), ('empty', 'empty', 'empty', 'empty', 'store_red'), ('empty', 'empty', 'empty', 'empty', 'store_blue')]\n"
     ]
    }
   ],
   "source": [
    "stored_item = ['empty', 'red', 'blue', 'white']\n",
    "all_states = list(product(stored_item, stored_item, stored_item, stored_item, all_actions))\n",
    "\n",
    "assert len(all_states) == 1536\n",
    "print(all_states[:10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model \n",
    "\n",
    "A model (sometimes called Transition Model) is a transition function P, which predicts the next state or the dynamics of the environnement. \n",
    "\n",
    "It tell us the probability distribution over next possible successor states, given the current state and the action taken by the agent.\n",
    "\n",
    "Pa(s, s') is the transition probability matrix with the probabilities to lead from state s into another state s' within the action a\n",
    "\n",
    "For the choice of probabilities we will base ourselves on the analysis of the training data that we carried out a little earlier. We were able to see a fairly even distribution for red and blue items and a larger distribution for red items.\n",
    "Another solution would also be to start with an equiprobability for each item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a 2*2 warehouse or a total of 4 slots, we can therefore number them from 0 to 3 in order to identify each slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_positions = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will now create all our probability transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]], dtype=float16)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transition_probability_matrix_with_distribution(tpm):\n",
    "    for index, row_vector in enumerate(tpm):\n",
    "        sum = np.sum(row_vector)\n",
    "        if sum == 0:\n",
    "            tpm[index, index] = 1\n",
    "            continue\n",
    "    return tpm\n",
    "\n",
    "\n",
    "def check_pick_up_action(agent_position, current_state, next_state):\n",
    "    item_color_to_pick_up = current_state[4].split(sep='_')[-1]\n",
    "    \n",
    "    # check if the wharehouse have already this item\n",
    "    if not item_color_to_pick_up in current_state[:4:]:\n",
    "        return False\n",
    "    \n",
    "    if current_state[agent_position] == item_color_to_pick_up and next_state[agent_position] == 'empty':\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "    \n",
    "\n",
    "def check_store_action(agent_position, current_state, next_state):\n",
    "    if not 'empty' in current_state:\n",
    "        return False\n",
    "    \n",
    "    item_color_to_store = current_state[4].split(sep='_')[-1]\n",
    "    \n",
    "    if current_state[agent_position] == 'empty' and next_state[agent_position] == item_color_to_store:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def check_transition(agent_position, current_state, next_state):\n",
    "    current_action = current_state[4]\n",
    "    if current_action == 'pick_up_red' or current_action == 'pick_up_blue' or current_action == 'pick_up_white':\n",
    "        return check_pick_up_action(agent_position, current_state, next_state)\n",
    "    else:\n",
    "        return check_store_action(agent_position, current_state, next_state)\n",
    "\n",
    "\n",
    "all_tpm = []\n",
    "for agent_position in slot_positions:\n",
    "    tpm = np.zeros((len(all_states), len(all_states)), dtype=np.float16)\n",
    "    for i, current_state in enumerate(all_states, start=0):\n",
    "        for j, next_state in enumerate(all_states, start=0):\n",
    "            if check_transition(agent_position, current_state, next_state):\n",
    "                next_action = next_state[4]\n",
    "                if next_action == 'store_red':\n",
    "                    tpm[i, j] = distribution_training.loc[(distribution_training['action'] == 'store') & (distribution_training['color'] == 'red'), 'count'].item()\n",
    "                elif next_action == 'store_white':\n",
    "                    tpm[i, j] = distribution_training.loc[(distribution_training['action'] == 'store') & (distribution_training['color'] == 'white'), 'count'].item()\n",
    "                elif next_action == 'store_blue':\n",
    "                    tpm[i, j] = distribution_training.loc[(distribution_training['action'] == 'store') & (distribution_training['color'] == 'blue'), 'count'].item()\n",
    "                elif next_action == 'pick_up_red':\n",
    "                    tpm[i, j] = distribution_training.loc[(distribution_training['action'] == 'restore') & (distribution_training['color'] == 'red'), 'count'].item()\n",
    "                elif next_action == 'pick_up_white':\n",
    "                    tpm[i, j] = distribution_training.loc[(distribution_training['action'] == 'restore') & (distribution_training['color'] == 'white'), 'count'].item()\n",
    "                else:\n",
    "                    tpm[i, j] = distribution_training.loc[(distribution_training['action'] == 'restore') & (distribution_training['color'] == 'blue'), 'count'].item()\n",
    "        \n",
    "    tpm_with_distribution = transition_probability_matrix_with_distribution(tpm)\n",
    "    all_tpm.append(tpm_with_distribution)\n",
    "\n",
    "print(len(all_tpm))\n",
    "all_tpm[:1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reward\n",
    "\n",
    "A Reward Rt is a scalar feedback signal that indicates how well the agent is doing at time step t. The agent's job is to maximize the expected sum of rewards. \n",
    "\n",
    "A Reward is a real-valued reward function. R(s) indicates the reward for simply being in the state S. \n",
    "\n",
    "R(S,a) indicates the reward for being in a state S and taking an action ‘a’. \n",
    "\n",
    "R(S,a,S’) indicates the reward for being in a state S, taking an action ‘a’ and ending up in a state S’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]\n",
      " ...\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "rewards_matrix = np.zeros((len(all_states), len(slot_positions)))\n",
    "for position in slot_positions:\n",
    "    for i, current_state in enumerate(all_states, start=0):\n",
    "        if current_state[position] == 'empty':\n",
    "            rewards_matrix[i, position] = position + 1\n",
    "\n",
    "print(rewards_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Policy \n",
    "\n",
    "A Policy is a solution to the Markov Decision Process. A policy is a mapping from S to a. It indicates the action ‘a’ to be taken while in state S.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mdptoolbox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4d3b2a178943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmdpResultPolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdptoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicyIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmdpResultValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdptoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValueIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run the MDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmdpResultPolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mdptoolbox' is not defined"
     ]
    }
   ],
   "source": [
    "mdpResultPolicy = mdptoolbox.mdp.PolicyIteration(tpm, reward_matrix, 0.2, max_iter=len(states) * 5)\n",
    "mdpResultValue = mdptoolbox.mdp.ValueIteration(tpm, reward_matrix, 0.2, max_iter=len(states) * 5)\n",
    "\n",
    "# Run the MDP\n",
    "mdpResultPolicy.run()\n",
    "mdpResultValue.run()\n",
    "\n",
    "print('PolicyIteration:')\n",
    "print(mdpResultPolicy.policy)\n",
    "print(mdpResultPolicy.V)\n",
    "print(mdpResultPolicy.iter)\n",
    "\n",
    "print('ValueIteration:')\n",
    "print(mdpResultValue.policy)\n",
    "print(mdpResultValue.V)\n",
    "print(mdpResultValue.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the sources used for the realization of this article...\n",
    "\n",
    "References :\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14\">Reinforcement Learning Demystified</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/reinforcement-learning-an-introduction-to-the-concepts-applications-and-code-ced6fbfd882d\">Reinforcement Learning: An Introduction to the Concepts, Applications and Code</a>\n",
    "\n",
    "<a href=\"https://medium.com/coinmonks/implement-reinforcement-learning-using-markov-decision-process-tutorial-272012fdae51\">Implement Reinforcement learning using Markov Decision Process</a>\n",
    "\n",
    "<a href=\"http://www.cs.cmu.edu/~10601b/slides/MDP_RL.pdf\">Markov Decision Process and Reinforcement Learning</a>\n",
    "\n",
    "<a href=\"https://hub.packtpub.com/reinforcement-learning-mdp-markov-decision-process-tutorial/\">Implement Reinforcement learning using Markov Decision Process [Tutorial]</a>\n",
    "\n",
    "<a href=\"https://www.geeksforgeeks.org/markov-decision-process/\">Markov Decision Process</a>\n",
    "\n",
    "<a href=\"http://incompleteideas.net/book/bookdraft2017nov5.pdf\">SB11 - Sutton Barto - Book - Reinforcement Learning.pdf</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
